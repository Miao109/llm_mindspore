{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  2.1.0\n",
      "The result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "# mindspore.set_context(device_target='CPU')\n",
    "# mindspore.set_context(device_target='GPU')\n",
    "mindspore.set_context(device_target=\"Ascend\")\n",
    "mindspore.set_context(device_id=0)\n",
    "mindspore.run_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 指令微调\n",
    "\n",
    "<img src=\"./images_llm/fig7.1.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 指令微调简介\n",
    "\n",
    "- 预训练 LLM 的过程是一个逐词生成单词的训练过程。在这个过程中，模型通过预测下一个词来学习语言模式和知识，但这种训练方式并未明确教会模型如何理解和执行用户的特定指令。\n",
    "\n",
    "- 预训练的LLM通常难以执行特定指令，因为它们主要学习的是语言的统计模式，而非如何响应用户的具体需求。例如，当被要求\"总结这篇文章\"或\"用法语翻译这个句子\"时，预训练模型可能会继续生成关于总结或翻译的文本，而不是执行实际的总结或翻译任务。\n",
    "\n",
    "- 本章的核心目标是提高LLM遵循特定指令并生成所需响应的能力。通过指令微调，我们将教会模型识别指令、理解其含义，并生成符合指令要求的恰当回应，从而使模型更加实用和符合人类期望的交互方式。\n",
    "\n",
    "\n",
    "<img src=\"./images_llm/fig7.2.svg\" width=\"600\">\n",
    "\n",
    "本章涵盖的主题如图所示。\n",
    "\n",
    "<img src=\"./images_llm/fig7.3.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 准备指令微调的数据集\n",
    "\n",
    "为了进行指令微调，我们需要准备一个由指令-响应对组成的数据集。这些数据通常通过人工编写或从现有对话中提取而来，它们对于训练模型理解和执行各种类型的指令至关重要。高质量的指令数据集能够显著提升模型在各种任务上的表现能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "\n",
    "    # The book originally contained this unnecessary \"else\" clause:\n",
    "    #else:\n",
    "    #    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    #        text_data = file.read()\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "file_path = \"instruction-data.json\"\n",
    "url = (\n",
    "    \"https://raw.githubusercontent.com/Miao109/llm_mindspore/refs/heads/main/instruction-data.json\"\n",
    ")\n",
    "\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data`列表中的每个条目时一个字典对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`input`字段可以为空值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指令微调的过程是在指令数据集上对模型进行训练，这些数据集中明确提供了输入-输出对。这与传统预训练中简单预测下一个词的任务不同，指令微调要求模型学习理解指令的含义并给出适当的响应。有多种方法可以为LLM格式化这些条目。\n",
    "\n",
    "\n",
    "<img src=\"./images_llm/fig7.4.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个format_input函数，用来将数据集中的条目格式化为Alpaca风格的输入样式。Alpaca是斯坦福大学开发的一种指令微调方法，其输入格式已被广泛采用：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "格式化结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果`input`字段为空值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先将数据集划分为训练集、验证集和测试集。在实际应用中，我们通常使用大部分数据进行训练，小部分用于验证和测试，以评估模型的泛化能力。这里我们使用85%的数据作为训练集，10%作为测试集，5%作为验证集。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:100]\n",
    "train_portion = int(len(data) * 0.85)  # 85% for training\n",
    "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
    "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training set length:\", len(train_data))\n",
    "print(\"Validation set length:\", len(val_data))\n",
    "print(\"Test set length:\", len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 将数据组织成训练批次\n",
    "\n",
    "有效的批处理是提高训练效率的关键。为了实现这一点，我们需要将数据转换为适合神经网络处理的格式，包括分词化和添加特殊标记等处理步骤。\n",
    "\n",
    "\n",
    "<img src=\"./images_llm/fig7.5.svg\" width=\"500\">\n",
    "\n",
    "批处理过程的实现包含五个子步骤：\n",
    "1. 应用提示模板：将原始指令数据转换为结构化的提示格式\n",
    "2. 分词化：将文本转换为模型能理解的数字ID序列\n",
    "3. 添加填充标记：确保每个批次中的样本长度一致\n",
    "4. 生成目标词元ID：创建模型应该预测的输出目标\n",
    "5. 用一个占位符（如-100）替换损失函数中的填充标记：确保填充部分不影响模型学习\n",
    "\n",
    "<img src=\"./images_llm/fig7.6.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先编写一个InstructionDataset类实现批处理过程的前两个步骤：使用特定的提示模板对条目进行格式化(2.1)，然后对其进行分词(2.2)，从而产生模型可以处理的词元ID序列。\n",
    "\n",
    "\n",
    "<img src=\"./images_llm/fig7.7.svg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "# from torch.utils.data import Dataset\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "\n",
    "class InstructionDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "\n",
    "        # Pre-tokenize texts\n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.encoded_texts[index], ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与分类微调一样，我们使用`<|endoftext|>`标记作为填充标记。这个特殊标记在GPT系列模型中用来表示文本的结束，同时也适合作为填充标记，因为它在预训练过程中就已经被模型学习过了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以开发一个自定义的collate函数执行流程中的填充步骤，该函数可传递给数据加载器。这个自定义collate函数将每个批次中的训练示例填充到相同的长度，允许不同批次具有不同的长度，这样可以减少不必要的计算资源浪费。\n",
    "\n",
    "<img src='./images_llm/fig7.8.svg' width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "def custom_collate_draft_1(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    # device=\"cpu\"\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    # and increase the max length by +1, which will add one extra\n",
    "    # padding token below\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst = []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to batch_max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        # Via padded[:-1], we remove the extra padded token\n",
    "        # that has been added via the +1 setting in batch_max_length\n",
    "        # (the extra padding token will be relevant in later codes)\n",
    "        inputs = padded[:-1]\n",
    "        inputs_lst.append(inputs)\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = numpy.stack(inputs_lst)\n",
    "    return inputs_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [    5     6 50256 50256 50256]\n",
      " [    7     8     9 50256 50256]]\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images_llm/fig7.9.svg' width=\"600\">\n",
    "\n",
    "上述自定义collate函数可用于从输入数据中创建批次数据。为了训练模型并计算损失以更新权重，我们还需要创建与输入ID批次相对应的目标词元ID批次。在语言模型训练中，目标通常是输入序列向右偏移一个位置的结果，这是因为模型的任务是预测序列中的下一个词元。\n",
    "\n",
    "<img src='./images_llm/fig7.10.svg' width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_draft_2(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item)+1 for item in batch)\n",
    "\n",
    "    # Pad and prepare inputs\n",
    "    inputs_lst, targets_lst = [], []\n",
    "\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        # Add an <|endoftext|> token\n",
    "        new_item += [pad_token_id]\n",
    "        # Pad sequences to max_length\n",
    "        padded = (\n",
    "            new_item + [pad_token_id] *\n",
    "            (batch_max_length - len(new_item))\n",
    "        )\n",
    "        inputs = padded[:-1]  # Truncate the last token for inputs\n",
    "        targets = padded[1:]  # Shift +1 to the right for targets\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    # Convert list of inputs to tensor and transfer to target device\n",
    "    inputs_tensor = numpy.stack(inputs_lst)\n",
    "    targets_tensor = numpy.stack(targets_lst)\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [    5     6 50256 50256 50256]\n",
      " [    7     8     9 50256 50256]]\n",
      "[[    1     2     3     4 50256]\n",
      " [    6 50256 50256 50256 50256]\n",
      " [    8     9 50256 50256 50256]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步，我们需要为所有填充标记分配一个-100占位符(ignore_index)。在计算损失时，这个特殊值能够将这些填充标记有效地排除在外，确保只有有意义的数据才会对模型的学习过程产生影响。这一步非常重要，因为如果不排除填充标记，模型会浪费资源学习预测填充标记，而这并不是我们期望的行为。\n",
    "\n",
    "<img src=\"./images_llm/fig7.11.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里在目标列表中保留了一个文本结束标记，ID为50256。保留这个标记的目的是让LLM学习何时根据指令生成文本结束标记，我们将其视为生成的响应已完成的指标。这对于实际应用中控制模型输出长度非常重要。\n",
    "\n",
    "<img src=\"./images_llm/fig7.12.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def custom_collate_fn(\n",
    "    batch, batchinfo=None,\n",
    "    pad_token_id=50256,\n",
    "    ignore_index=-100,\n",
    "    allowed_max_length=None,\n",
    "):\n",
    "    # Find the longest sequence in the batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    # print(\"batch_max_length:\", batch_max_length)\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item = np.pad(new_item, (0, 1), constant_values=pad_token_id)\n",
    "        padding_length = batch_max_length - len(new_item)\n",
    "        padded = np.pad(new_item, (0, padding_length), mode=\"constant\", constant_values=pad_token_id)\n",
    "        padded = padded.tolist()\n",
    "        inputs = np.array(padded[:-1])  # Truncate the last token for inputs\n",
    "        targets = np.array(padded[1:])  # Shift +1 to the right for targets\n",
    "\n",
    "        mask = (targets == pad_token_id)\n",
    "        indices = np.nonzero(mask)[0]\n",
    "        num = indices.size\n",
    "        if num > 1:\n",
    "            for i in indices[1:]:\n",
    "                targets[i] = ignore_index\n",
    "        # New: Optionally truncate to maximum sequence length\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "        # inputs_lst = ops.stack(inputs_lst)\n",
    "        # targets_lst = ops.stack(targets_lst)\n",
    "    return np.array(inputs_lst), np.array(targets_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     1     2     3     4]\n",
      " [    5     6 50256 50256 50256]\n",
      " [    7     8     9 50256 50256]]\n",
      "[[    1     2     3     4 50256]\n",
      " [    6 50256  -100  -100  -100]\n",
      " [    8     9 50256  -100  -100]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更直观地演示，考虑以下示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1269281\n"
     ]
    }
   ],
   "source": [
    "import mindspore.ops as ops\n",
    "logits_1 = mindspore.Tensor(\n",
    "    [[-1.0, 1.0],  # 1st training example\n",
    "     [-0.5, 1.5]]  # 2nd training example\n",
    ")\n",
    "targets_1 = mindspore.Tensor([0, 1])\n",
    "\n",
    "\n",
    "loss_1 = ops.cross_entropy(logits_1, targets_1)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加第三个示例文本后，得到的损失值为 0.7936。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7935948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits_2 = mindspore.Tensor(\n",
    "    [[-1.0, 1.0],\n",
    "     [-0.5, 1.5],\n",
    "     [-0.5, 1.5]]  # New 3rd training example\n",
    ")\n",
    "targets_2 = mindspore.Tensor([0, 1, 1])\n",
    "\n",
    "loss_2 = ops.cross_entropy(logits_2, targets_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1269281\n",
      "loss_1 == loss_3: True\n"
     ]
    }
   ],
   "source": [
    "targets_3 = mindspore.Tensor([0, 1, -100])\n",
    "\n",
    "loss_3 = ops.cross_entropy(logits_2, targets_3)\n",
    "print(loss_3)\n",
    "print(\"loss_1 == loss_3:\", loss_1 == loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵函数`cross_entropy`中的参数`ignore_index`的值默认为$-100$，这一设置使得函数在计算损失时，会自动忽略所有被标记为$-100$的目标值。这个特性允许我们在计算损失时只关注那些有意义的预测，而不是浪费计算资源在填充标记上。\n",
    "\n",
    "除了屏蔽填充标记之外，屏蔽与指令相对应的目标词元ID也很常见。通过屏蔽与指令相对应的LLM目标词元ID，交叉熵损失仅针对生成的响应目标ID进行计算。因此，模型被训练为专注于生成准确的响应而不是记忆指令，这有助于减少过度拟合，提高模型在新指令上的泛化能力。\n",
    "\n",
    "<img src=\"./images_llm/fig7.13.svg\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 创建数据加载器\n",
    "\n",
    "数据加载器负责将数据以批次的形式提供给模型进行训练，它是连接数据处理和模型训练的重要桥梁。通过适当配置数据加载器，我们可以实现数据的高效加载和处理。\n",
    "\n",
    "<img src=\"./images_llm/fig7.14.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functools.partial 可以​“冻结”函数的部分参数，生成一个新的简化版函数。新函数在调用时，可以省去重复传递这些已固定的参数。这在数据加载过程中特别有用，可以保持代码的简洁性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    allowed_max_length=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_workers = 1\n",
    "batch_size = 8\n",
    "\n",
    "# mindspore.set_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = GeneratorDataset(train_dataset,  [\"input_ids\"], shuffle=True)\n",
    "train_loader = train_loader.batch(\n",
    "    batch_size=batch_size,\n",
    "    per_batch_map=customized_collate_fn,\n",
    "    drop_remainder=True,\n",
    "    num_parallel_workers=num_workers,\n",
    "    output_columns=[\"input_ids\", \"target_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = GeneratorDataset(val_dataset, ['input_ids'], shuffle=False)\n",
    "val_loader = val_loader.batch(\n",
    "    batch_size=batch_size,\n",
    "    per_batch_map=customized_collate_fn,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_workers=num_workers,\n",
    "    output_columns=[\"input_ids\", \"target_ids\"]\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = GeneratorDataset(val_dataset, ['input_ids'], shuffle=False)\n",
    "test_loader = test_loader.batch(\n",
    "    batch_size=batch_size,\n",
    "    per_batch_map=customized_collate_fn,\n",
    "    drop_remainder=False,\n",
    "    num_parallel_workers=num_workers,\n",
    "    output_columns=[\"input_ids\", \"target_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查一下加载器生成的输入和目标批次的维度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "(8, 77) (8, 77)\n",
      "(8, 67) (8, 67)\n",
      "(8, 68) (8, 68)\n",
      "(8, 69) (8, 69)\n",
      "(8, 65) (8, 65)\n",
      "(8, 64) (8, 64)\n",
      "(8, 61) (8, 61)\n",
      "(8, 66) (8, 66)\n",
      "(8, 64) (8, 64)\n",
      "(8, 61) (8, 61)\n",
      "(8, 80) (8, 80)\n",
      "(8, 62) (8, 62)\n",
      "(8, 62) (8, 62)\n",
      "(8, 83) (8, 83)\n",
      "(8, 60) (8, 60)\n",
      "(8, 65) (8, 65)\n",
      "(8, 65) (8, 65)\n",
      "(8, 72) (8, 72)\n",
      "(8, 70) (8, 70)\n",
      "(8, 65) (8, 65)\n",
      "(8, 70) (8, 70)\n",
      "(8, 68) (8, 68)\n",
      "(8, 62) (8, 62)\n",
      "(8, 75) (8, 75)\n",
      "(8, 75) (8, 75)\n",
      "(8, 64) (8, 64)\n",
      "(8, 61) (8, 61)\n",
      "(8, 72) (8, 72)\n",
      "(8, 74) (8, 74)\n",
      "(8, 64) (8, 64)\n",
      "(8, 76) (8, 76)\n",
      "(8, 60) (8, 60)\n",
      "(8, 82) (8, 82)\n",
      "(8, 63) (8, 63)\n",
      "(8, 67) (8, 67)\n",
      "(8, 83) (8, 83)\n",
      "(8, 68) (8, 68)\n",
      "(8, 69) (8, 69)\n",
      "(8, 61) (8, 61)\n",
      "(8, 73) (8, 73)\n",
      "(8, 83) (8, 83)\n",
      "(8, 56) (8, 56)\n",
      "(8, 76) (8, 76)\n",
      "(8, 65) (8, 65)\n",
      "(8, 83) (8, 83)\n",
      "(8, 61) (8, 61)\n",
      "(8, 64) (8, 64)\n",
      "(8, 69) (8, 69)\n",
      "(8, 68) (8, 68)\n",
      "(8, 71) (8, 71)\n",
      "(8, 75) (8, 75)\n",
      "(8, 62) (8, 62)\n",
      "(8, 77) (8, 77)\n",
      "(8, 69) (8, 69)\n",
      "(8, 76) (8, 76)\n",
      "(8, 80) (8, 80)\n",
      "(8, 88) (8, 88)\n",
      "(8, 60) (8, 60)\n",
      "(8, 71) (8, 71)\n",
      "(8, 68) (8, 68)\n",
      "(8, 62) (8, 62)\n",
      "(8, 70) (8, 70)\n",
      "(8, 83) (8, 83)\n",
      "(8, 59) (8, 59)\n",
      "(8, 68) (8, 68)\n",
      "(8, 71) (8, 71)\n",
      "(8, 67) (8, 67)\n",
      "(8, 64) (8, 64)\n",
      "(8, 60) (8, 60)\n",
      "(8, 68) (8, 68)\n",
      "(8, 83) (8, 83)\n",
      "(8, 71) (8, 71)\n",
      "(8, 59) (8, 59)\n",
      "(8, 69) (8, 69)\n",
      "(8, 64) (8, 64)\n",
      "(8, 69) (8, 69)\n",
      "(8, 66) (8, 66)\n",
      "(8, 62) (8, 62)\n",
      "(8, 68) (8, 68)\n",
      "(8, 66) (8, 66)\n",
      "(8, 87) (8, 87)\n",
      "(8, 62) (8, 62)\n",
      "(8, 64) (8, 64)\n",
      "(8, 72) (8, 72)\n",
      "(8, 91) (8, 91)\n",
      "(8, 67) (8, 67)\n",
      "(8, 65) (8, 65)\n",
      "(8, 71) (8, 71)\n",
      "(8, 72) (8, 72)\n",
      "(8, 89) (8, 89)\n",
      "(8, 70) (8, 70)\n",
      "(8, 66) (8, 66)\n",
      "(8, 91) (8, 91)\n",
      "(8, 68) (8, 68)\n",
      "(8, 74) (8, 74)\n",
      "(8, 66) (8, 66)\n",
      "(8, 91) (8, 91)\n",
      "(8, 60) (8, 60)\n",
      "(8, 65) (8, 65)\n",
      "(8, 70) (8, 70)\n",
      "(8, 80) (8, 80)\n",
      "(8, 79) (8, 79)\n",
      "(8, 80) (8, 80)\n",
      "(8, 62) (8, 62)\n",
      "(8, 73) (8, 73)\n",
      "(8, 66) (8, 66)\n",
      "(8, 67) (8, 67)\n",
      "(8, 74) (8, 74)\n",
      "(8, 73) (8, 73)\n",
      "(8, 78) (8, 78)\n",
      "(8, 68) (8, 68)\n",
      "(8, 68) (8, 68)\n",
      "(8, 81) (8, 81)\n",
      "(8, 60) (8, 60)\n",
      "(8, 66) (8, 66)\n",
      "(8, 69) (8, 69)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21106   318   281 12064   326  8477   257  4876    13 19430   257  2882\n",
      "   326 20431 32543   262  2581    13   198   198 21017 46486    25   198\n",
      "  8645   378   257  6827  1262   262  1573   705   325 10920   541   414\n",
      "  2637   198   198 21017 18261    25   198 36276   262  7104 10481   373\n",
      "  5899   384 10920   541   414    13 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  318   281 12064   326  8477   257  4876    13 19430   257  2882   326\n",
      " 20431 32543   262  2581    13   198   198 21017 46486    25   198  8645\n",
      "   378   257  6827  1262   262  1573   705   325 10920   541   414  2637\n",
      "   198   198 21017 18261    25   198 36276   262  7104 10481   373  5899\n",
      "   384 10920   541   414    13 50256  -100  -100  -100  -100  -100  -100\n",
      "  -100  -100  -100  -100  -100  -100  -100  -100  -100]\n"
     ]
    }
   ],
   "source": [
    "print(targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 加载预训练模型\n",
    "\n",
    "指令微调是建立在预训练模型基础上的，因此我们需要先加载一个已经经过大规模预训练的模型，然后在此基础上进行微调。这种方法能够有效利用预训练模型已经学到的语言知识，大大减少了训练所需的计算资源和数据量。\n",
    "\n",
    "<img src=\"./images_llm/fig7.15.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:25:48.665.785 [mindspore/nn/layer/basic.py:171] This parameter `dtype` will be deleted or invisible in the future. Please don't use it.\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel<\n",
       "  (tok_emb): Embedding<vocab_size=50257, embedding_size=1024, use_one_hot=False, embedding_table=Parameter (name=embedding_table, shape=(50257, 1024), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (pos_emb): Embedding<vocab_size=1024, embedding_size=1024, use_one_hot=False, embedding_table=Parameter (name=embedding_table, shape=(1024, 1024), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (drop_emb): Dropout<p=0.0>\n",
       "  (trf_blocks): SequentialCell<\n",
       "    (0): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (1): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (2): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (3): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (4): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (5): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (6): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (7): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (8): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (9): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (10): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (11): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (12): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (13): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (14): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (15): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (16): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (17): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (18): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (19): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (20): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (21): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (22): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    (23): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_key): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (W_value): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=1024, output_channels=1024, has_bias=True>\n",
       "        (dropout): Dropout<p=0.0>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=1024, output_channels=4096, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=4096, output_channels=1024, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.0>\n",
       "      >\n",
       "    >\n",
       "  (final_norm): LayerNorm<>\n",
       "  (out_head): Dense<input_channels=1024, output_channels=50257>\n",
       "  >"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(model, params)\n",
    "model.set_train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "# mindspore.set_seed(123)\n",
    "\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "     # 检查是否有批量维度，如果有则移除\n",
    "    if len(token_ids.shape) > 1 and token_ids.shape[0] == 1:\n",
    "        flat = token_ids.squeeze(0)\n",
    "    else:\n",
    "        flat = token_ids\n",
    "    # flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256,\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate 函数返回的是输入和输出文本的组合。由于预训练的 LLM 主要设计为文本续写，输入和输出连接在一起可以形成一个连贯且清晰的文本。但是，在特定任务上评估模型的表现时，我们通常只想关注模型生成的响应。\n",
    "\n",
    "为了单独获取模型的响应文本，我们需要从`generated_text`的总长度中去除输入指令："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "response_text = (\n",
    "    generated_text[len(input_text):]\n",
    "    .replace(\"### Response:\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，微调前的模型还未能正确理解和执行指令，只是简单地复述了部分输入文本。这正是我们需要进行指令微调的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 指令微调\n",
    "\n",
    "现在我们已经准备好了数据和模型，可以开始进行指令微调过程。通过微调，模型将学习如何识别指令并生成合适的响应，从而更好地满足用户需求。\n",
    "\n",
    "<img src=\"./images_llm/fig7.16.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import value_and_grad\n",
    "def calc_loss_loader(data_loader, model, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "def calc_loss_batch(input_batch, target_batch, model):\n",
    "    logits = model(input_batch)\n",
    "    loss = ops.cross_entropy(ops.flatten(logits, start_dim=0, end_dim=1), target_batch.flatten())\n",
    "    return loss\n",
    "def evaluate_model(model, train_loader, val_loader, eval_iter):\n",
    "    model.set_train(False)\n",
    "    train_loss = calc_loss_loader(train_loader, model, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, num_batches=eval_iter)\n",
    "    model.set_train(True)\n",
    "    return train_loss, val_loss\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # 定义前向传播函数\n",
    "    def forward_fn(input_batch, target_batch):\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "        return loss\n",
    "    \n",
    "    # 获取计算梯度的函数\n",
    "    grad_fn = value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.set_train(True)  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            # 计算损失和梯度\n",
    "            loss, grads = grad_fn(input_batch, target_batch)\n",
    "            # 使用优化器更新参数\n",
    "            optimizer(grads)\n",
    "\n",
    "            # loss.backward() # Calculate loss gradients\n",
    "            # optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss.asnumpy():.3f}, Val loss {val_loss.asnumpy():.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "def generate_and_print_sample(model, tokenizer, start_context):\n",
    "    model.set_train(False)\n",
    "    context_size = model.pos_emb.embedding_table.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.set_train(True)\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        # with mindspore.context.grad_off():\n",
    "        logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        softmax = ops.Softmax(axis=-1)\n",
    "        probas = softmax(logits)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = ops.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = ops.concat((idx, idx_next), axis=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.720142\n",
      "Validation loss: 3.7619357\n"
     ]
    }
   ],
   "source": [
    "train_loss = calc_loss_loader(train_loader, model, num_batches=5)\n",
    "val_loss = calc_loss_loader(val_loader, model, num_batches=5)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们设置一个合理的学习率和权重衰减值来进行微调。对于指令微调任务，通常使用较小的学习率（如0.00005）以避免破坏预训练模型已经学到的知识。同时，添加适当的权重衰减（如0.1）可以防止模型过度拟合训练数据。\n",
    "\n",
    "微调过程中，我们会定期评估模型在训练集和验证集上的表现，以监控训练进度并防止过拟合。此外，我们还会在每个训练周期结束后生成一个示例，直观地观察模型响应指令的能力是否有所提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 2.729, Val loss 2.663\n",
      "|\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(260517,ffff8d497b70,python):2025-03-13-21:11:54.001.760 [mindspore/ccsrc/transform/graph_ir/utils.cc:70] FindAdapter] Can't find OpAdapter for GatherDGradV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000005): Train loss 2.028, Val loss 2.079\n",
      "Ep 1 (Step 000010): Train loss 2.053, Val loss 2.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(260517,ffff8d497b70,python):2025-03-13-21:16:18.693.998 [mindspore/ccsrc/transform/graph_ir/utils.cc:70] FindAdapter] Can't find OpAdapter for GatherDGradV2\n",
      "[WARNING] GE_ADPT(260517,ffff8d497b70,python):2025-03-13-21:16:24.781.039 [mindspore/ccsrc/transform/graph_ir/utils.cc:70] FindAdapter] Can't find OpAdapter for GatherDGradV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000015): Train loss 2.073, Val loss 2.061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(260517,ffff8d497b70,python):2025-03-13-21:18:06.292.810 [mindspore/ccsrc/transform/graph_ir/utils.cc:70] FindAdapter] Can't find OpAdapter for GatherDGradV2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000020): Train loss 2.141, Val loss 2.108\n",
      "Ep 1 (Step 000025): Train loss 2.211, Val loss 2.218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] GE_ADPT(260517,ffff8d497b70,python):2025-03-13-21:22:24.830.252 [mindspore/ccsrc/transform/graph_ir/utils.cc:70] FindAdapter] Can't find OpAdapter for GatherDGradV2\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import mindspore.nn as nn\n",
    "start_time = time.time()\n",
    "\n",
    "# optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=0.00005, weight_decay=0.1)\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=0.00005)\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
    "print(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = ops.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 提取并保存模型响应\n",
    "\n",
    "完成微调后，我们需要评估模型的表现，并将训练好的模型保存起来以便未来使用。评估通常包括对测试集数据的定量和定性分析。\n",
    "\n",
    "<img src=\"./images_llm/fig7.17.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.281.974 [mindspore/train/serialization.py:170] The type of trf_blocks.0.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.341.971 [mindspore/train/serialization.py:170] The type of trf_blocks.1.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.404.846 [mindspore/train/serialization.py:170] The type of trf_blocks.2.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.466.093 [mindspore/train/serialization.py:170] The type of trf_blocks.3.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.529.663 [mindspore/train/serialization.py:170] The type of trf_blocks.4.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.595.330 [mindspore/train/serialization.py:170] The type of trf_blocks.5.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.664.494 [mindspore/train/serialization.py:170] The type of trf_blocks.6.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.728.536 [mindspore/train/serialization.py:170] The type of trf_blocks.7.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.787.041 [mindspore/train/serialization.py:170] The type of trf_blocks.8.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.845.076 [mindspore/train/serialization.py:170] The type of trf_blocks.9.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.905.549 [mindspore/train/serialization.py:170] The type of trf_blocks.10.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:07.973.655 [mindspore/train/serialization.py:170] The type of trf_blocks.11.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.898.13 [mindspore/train/serialization.py:170] The type of trf_blocks.12.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.211.883 [mindspore/train/serialization.py:170] The type of trf_blocks.13.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.341.326 [mindspore/train/serialization.py:170] The type of trf_blocks.14.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.444.026 [mindspore/train/serialization.py:170] The type of trf_blocks.15.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.542.125 [mindspore/train/serialization.py:170] The type of trf_blocks.16.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.642.924 [mindspore/train/serialization.py:170] The type of trf_blocks.17.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.744.882 [mindspore/train/serialization.py:170] The type of trf_blocks.18.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.840.321 [mindspore/train/serialization.py:170] The type of trf_blocks.19.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:08.949.625 [mindspore/train/serialization.py:170] The type of trf_blocks.20.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:09.496.35 [mindspore/train/serialization.py:170] The type of trf_blocks.21.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:09.156.633 [mindspore/train/serialization.py:170] The type of trf_blocks.22.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n",
      "[WARNING] ME(309179:281472827042672,MainProcess):2025-03-13-21:28:09.288.166 [mindspore/train/serialization.py:170] The type of trf_blocks.23.att.mask:Float64 in 'parameter_dict' is different from the type of it in 'net':Float32, then the type convert from Float64 to Float32 in the network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from gpt_model import GPTModel\n",
    "import mindspore\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "\n",
    "param_dict = mindspore.load_checkpoint(\"model_instruction_sft.ckpt\")\n",
    "param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import (\n",
    "    generate,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n",
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> A type of cloud is typically associated with thunderstorms.\n",
      "-------------------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for entry in test_data[:3]:\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = (\n",
    "        generated_text[len(input_text):]\n",
    "        .replace(\"### Response:\", \"\")\n",
    "        .strip()\n",
    ")\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际应用中，指令微调后的模型相比原始预训练模型，通常表现出显著更强的指令理解和执行能力。这种改进使得模型能够更好地应对实际应用场景中的各种任务要求，为用户提供更有价值的服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们需要将训练好的模型保存下来，以便在未来直接加载使用，而无需重新进行训练。这不仅节省了计算资源，也使得模型能够更方便地部署到生产环境中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
    "mindspore.save_checkpoint(model, file_name)\n",
    "print(f\"Model saved as {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.15.0",
   "language": "python",
   "name": "tensorflow-1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
