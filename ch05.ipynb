{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  2.1.0\n",
      "The result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "mindspore.set_context(device_target='CPU')\n",
    "# mindspore.set_context(device_target='GPU')\n",
    "mindspore.set_context(device_target=\"Ascend\")\n",
    "mindspore.set_context(device_id=0)\n",
    "mindspore.run_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 对未标记数据进行预训练\n",
    "\n",
    "截至目前，我们已经完成了数据采样和注意力机制的实现，并成功构建了LLM的架构。现在，是时候着手实现训练函数，并对LLM进行预训练了。预训练是构建大语言模型的关键步骤，它使模型能够从大量未标记文本中学习语言的结构和规律。\n",
    "\n",
    "<img src=\"./images_llm/fig5.1.svg\"  width='600'>\n",
    "\n",
    "本章涵盖的主题如下图所示，我们将从评估生成文本开始，随后探讨模型训练方法，最后介绍几种常用的解码策略：\n",
    "\n",
    "<img src=\"./images_llm/fig5.2.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 评估生成文本模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在开始训练之前，我们需要确立评估模型性能的指标和方法。这一节将介绍如何评估生成文本的质量，以及如何计算模型在训练和验证数据上的损失值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 使用GPT生成文本\n",
    "\n",
    "首先，使用如下配置初始化GPT模型。这里我们定义一个基本的GPT模型配置，包括词汇表大小、上下文长度、嵌入维度等参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "[WARNING] ME(98110:281473781230448,MainProcess):2025-03-13-18:20:50.374.704 [mindspore/nn/layer/basic.py:171] This parameter `dtype` will be deleted or invisible in the future. Please don't use it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel<\n",
       "  (tok_emb): Embedding<vocab_size=50257, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=tok_emb.embedding_table, shape=(50257, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (pos_emb): Embedding<vocab_size=256, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=pos_emb.embedding_table, shape=(256, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (drop_emb): Dropout<p=0.1>\n",
       "  (trf_blocks): SequentialCell<\n",
       "    (0): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (1): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (2): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (3): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (4): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (5): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (6): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (7): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (8): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (9): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (10): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (11): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    >\n",
       "  (final_norm): LayerNorm<>\n",
       "  (out_head): Dense<input_channels=768, output_channels=50257>\n",
       "  >"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mindspore\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "mindspore.set_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.set_train(False)  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的上下文长度（context_length）缩减为256个词元，这一修改有效降低了计算负担。完成我们的训练之后，将调整上下文大小，并加载上下文长度为1024的预训练模型权重。\n",
    "\n",
    "接着，我们使用GPTModel实例，结合第4章中的generate_text_simple函数生成文本，并应用text_to_token_ids和token_ids_to_text这两个函数实现文本与词元之间的转换。下图展示了这一过程的工作流：\n",
    "\n",
    "<img src=\"./images_llm/fig5.3.svg\" width='700'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from mindspore import Tensor\n",
    "\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "import mindspore.numpy as np\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        # with mindspore.context.grad_off():\n",
    "        logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        softmax = ops.Softmax(axis=-1)\n",
    "        probas = softmax(logits)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = ops.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = ops.concat((idx, idx_next), axis=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = Tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "     # 检查是否有批量维度，如果有则移除\n",
    "    if len(token_ids.shape) > 1 and token_ids.shape[0] == 1:\n",
    "        flat = token_ids.squeeze(0)\n",
    "    else:\n",
    "        flat = token_ids\n",
    "    # flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rhythm rhythmaruisleisleisle childish childish childish childish\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Every effort moves you\"\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，未经训练的模型无法生成连贯的文本。为了明确什么是\"连贯\"或\"高质量\"的文本，我们需要采用一种数值评估方法，使我们能够在整个训练过程中持续监控并提升模型的性能。接下来，我们将为生成的输出计算一个损失指标，这一指标将作为训练的衡量标准。此外，在后续的章节中，当我们对LLM进行微调时，还将介绍评估模型质量的其他方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 计算文本生成损失\n",
    "\n",
    "为了量化模型生成文本的质量，我们需要定义一个损失函数。在语言模型中，常用的损失函数是交叉熵损失，它衡量模型预测的概率分布与真实分布之间的差异。\n",
    "\n",
    "假设有2个训练示例，对应的目标张量为输入张量向右移动1个位置的结果。这反映了语言模型的核心任务：预测序列中的下一个词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = Tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们把数据输入到模型中，以计算出2个输入张量的logits向量。接着应用softmax函数将logits转换为概率向量，表示模型对每个可能的下一个词元的预测概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 50257)\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn\n",
    "logits = model(inputs)\n",
    "\n",
    "softmax = nn.Softmax(axis=-1)\n",
    "probas = softmax(logits) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图展示了一个LLM生成文本的总体流程，概述了如何将概率分数转换回文本。模型计算出词汇表中每个词元的概率分布，然后通过某种选择策略（如取最大概率值）来确定下一个词元。\n",
    "\n",
    "<img src=\"./images_llm/fig5.4.svg\" width='700'>\n",
    "\n",
    "将argmax函数应用于概率向量后，可以得到两组输出，每组包含3个预测的词元ID："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " [[[40978]\n",
      "  [32136]\n",
      "  [40189]]\n",
      "\n",
      " [[33768]\n",
      "  [ 2301]\n",
      "  [22235]]]\n"
     ]
    }
   ],
   "source": [
    "token_ids = ops.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对这些token ID进行解码，可以发现输出的token与期望模型生成的目标token存在显著差异。这表明未训练的模型还没有学会预测合理的下一个词元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: reth Moody inscription\n"
     ]
    }
   ],
   "source": [
    "res = token_ids_to_text(targets[0], tokenizer)\n",
    "print(f\"Targets batch 1: {res}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了完成模型的训练，我们需要知道预测文本与目标文本之间的距离。交叉熵损失提供了这一度量，它衡量预测概率分布与目标分布之间的差异。\n",
    "\n",
    "<img src=\"./images_llm/fig5.5.svg\" width='700'>\n",
    "\n",
    "3个目标词元ID对应的预测概率为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: [1.3572932e-05 1.2135792e-05 1.2359466e-05]\n",
      "Text 2: [1.1107936e-05 3.2941804e-05 3.0851545e-05]\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练LLM的目标是最大化正确词元的预测概率（接近1）。通过计算对数概率，我们可以更方便地处理极小的概率值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-11.207433 -11.319351 -11.301088 -11.40785  -10.320768 -10.386324]\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = ops.log(ops.concat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算平均对数概率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.99047\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = np.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练的目标是通过更新模型的权重，使得平均对数概率尽可能接近0（即预测概率接近1）。然而，在深度学习的实际应用中，我们通常会将对数损失函数乘以-1（即使用负对数概率），然后最小化这个值。这样处理可以使训练过程更加稳定，并与其他深度学习模型的损失函数保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.99047\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MindSpore中有一个内置的cross_entropy函数，能够一步完成上述六个步骤，极大地简化了损失计算的流程：\n",
    "\n",
    "<img src=\"./images_llm/fig5.6.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (2, 3, 50257)\n",
      "Targets shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: (6, 50257)\n",
      "Flattened targets: (6,)\n"
     ]
    }
   ],
   "source": [
    "logits_flat = ops.flatten(logits, start_dim=0, end_dim=1)\n",
    "targets_flat = ops.flatten(targets, start_dim=0, end_dim=1)\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.99047\n"
     ]
    }
   ],
   "source": [
    "loss = ops.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与交叉熵损失相关的一个重要概念是困惑度（perplexity）。困惑度是交叉熵损失的指数，它提供了另一种评估语言模型的方式。困惑度可以解释为模型在每个位置平均需要考虑的词汇数量。较低的困惑度表示模型更有信心预测正确的下一个词元，因此生成更流畅、更自然的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59306.25\n"
     ]
    }
   ],
   "source": [
    "perplexity = np.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 计算训练和验证集损失\n",
    "\n",
    "为了评估模型的泛化能力，我们需要在训练集和验证集上计算损失。这里使用了一个简短的文本数据集——the-verdict.txt："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/Miao109/llm_mindspore/refs/heads/main/the-verdict.txt?token=GHSAT0AAAAAADAOHYA7TYWCH2P3VLK7CS2KZ6SVODA\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        text_data = response.read().decode('utf-8')\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(text_data)\n",
    "else:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "快速查看这一数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "# First 100 characters\n",
    "print(text_data[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# Last 100 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这个数据集仅包含5,145个词元，相对较小。接下来，我们将把这个数据集划分为训练集和验证集，并利用第2章中的数据加载器为LLM训练准备数据。这里我们采用上下文窗口为GPT_CONFIG_124M[\"context_length\"]的滑动窗口方法，每个样本包含连续的词元序列。\n",
    "\n",
    "<img src=\"./images_llm/fig5.7.svg\" width='800'>\n",
    "\n",
    "考虑到我们处理的数据集规模较小，为了降低计算资源的需求，这里选择了相对较小的批处理大小（batch_size=2）。在实际大规模训练中，通常会使用更大的批处理大小以提高训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4612 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "mindspore.set_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "(2, 256) (2, 256)\n",
      "\n",
      "Validation loader:\n",
      "(2, 256) (2, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们实现一个辅助函数，用于计算特定批次数据上的交叉熵损失。calc_loss_batch函数计算单个批次的损失，而calc_loss_loader函数则计算整个数据加载器的平均损失，便于我们监控模型在训练和验证集上的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model):\n",
    "    logits = model(input_batch)\n",
    "    loss = ops.cross_entropy(ops.flatten(logits, start_dim=0, end_dim=1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以使用这些函数来计算模型在训练和验证集上的初始损失值，这将作为训练过程中的基准参考点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.996593 <class 'mindspore.common._stub_tensor.StubTensor'>\n",
      "Train loss 10.997, Val loss 10.983\n",
      "Validation loss: 10.983248\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "# with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "train_loss = calc_loss_loader(train_loader, model)\n",
    "val_loss = calc_loss_loader(val_loader, model)\n",
    "\n",
    "print(\"Training loss:\", train_loss, type(train_loss))\n",
    "print(f\"Train loss {train_loss.asnumpy():.3f}, Val loss {val_loss.asnumpy():.3f}\")\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 训练 LLM\n",
    "\n",
    "完成了评估准备工作后，我们现在可以实现预训练LLM的代码了。预训练过程涉及将模型暴露于大量未标记文本数据，使其学习预测序列中的下一个词元。\n",
    "\n",
    "<img src=\"./images_llm/fig5.8.svg\" width='600'>\n",
    "\n",
    "为了保持代码的简洁易读，我们将构建一个简单的训练循环。该循环包括前向传播计算损失、反向传播计算梯度、以及使用优化器更新模型参数三个关键步骤。\n",
    "\n",
    "<img src=\"./images_llm/fig5.9.svg\" width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import value_and_grad\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    # 定义前向传播函数\n",
    "    def forward_fn(input_batch, target_batch):\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "        return loss\n",
    "    \n",
    "    # 获取计算梯度的函数\n",
    "    grad_fn = value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.set_train(True)  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            # loss = calc_loss_batch(input_batch, target_batch, model)\n",
    "            # 计算损失和梯度\n",
    "            loss, grads = grad_fn(input_batch, target_batch)\n",
    "            \n",
    "            # 使用优化器更新参数\n",
    "            grads = ops.clip_by_global_norm(grads, clip_norm=0.5)  # 严格裁剪\n",
    "            optimizer(grads)\n",
    "\n",
    "            # loss.backward() # Calculate loss gradients\n",
    "            # optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss.asnumpy():.3f}, Val loss {val_loss.asnumpy():.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, eval_iter):\n",
    "    model.set_train(False)\n",
    "    train_loss = calc_loss_loader(train_loader, model, num_batches=eval_iter)\n",
    "    val_loss = calc_loss_loader(val_loader, model, num_batches=eval_iter)\n",
    "    model.set_train(True)\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, start_context):\n",
    "    model.set_train(False)\n",
    "    context_size = model.pos_emb.embedding_table.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer)\n",
    "    token_ids = generate_text_simple(\n",
    "        model=model, idx=encoded,\n",
    "        max_new_tokens=50, context_size=context_size\n",
    "    )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.set_train(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们使用AdamW优化器，并结合之前定义的train_model_simple函数，对LLM进行训练。AdamW是Adam优化器的一个变体，增加了权重衰减正则化，有助于减少过拟合。我们指定训练10个epoch，每5步评估一次模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(98110:281473781230448,MainProcess):2025-03-13-18:23:15.490.66 [mindspore/nn/layer/basic.py:171] This parameter `dtype` will be deleted or invisible in the future. Please don't use it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.077, Val loss 10.243\n",
      "Ep 1 (Step 000005): Train loss 8.581, Val loss 8.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you..................................................\n",
      "Ep 2 (Step 000010): Train loss 7.235, Val loss 7.482\n",
      "Ep 2 (Step 000015): Train loss 6.475, Val loss 6.804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 3 (Step 000020): Train loss 6.250, Val loss 6.608\n",
      "Ep 3 (Step 000025): Train loss 6.112, Val loss 6.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ep 4 (Step 000030): Train loss 6.066, Val loss 6.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 4 (Step 000035): Train loss 6.046, Val loss 6.671\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 5 (Step 000040): Train loss 5.999, Val loss 6.702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 6 (Step 000045): Train loss 6.009, Val loss 6.721\n",
      "Ep 6 (Step 000050): Train loss 5.969, Val loss 6.716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 7 (Step 000055): Train loss 6.003, Val loss 6.703\n",
      "Ep 7 (Step 000060): Train loss 5.901, Val loss 6.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 8 (Step 000065): Train loss 5.868, Val loss 6.669\n",
      "Ep 8 (Step 000070): Train loss 5.819, Val loss 6.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 9 (Step 000075): Train loss 5.799, Val loss 6.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 9 (Step 000080): Train loss 5.775, Val loss 6.622\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 10 (Step 000085): Train loss 5.720, Val loss 6.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Ep 11 (Step 000090): Train loss 5.745, Val loss 6.635\n",
      "Ep 11 (Step 000095): Train loss 5.669, Val loss 6.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, the the the the the the the the the the the, the the the the the,, the the the the, the the the the the, the,, the the,,,, the the,,, the the the the the\n",
      "Ep 12 (Step 000100): Train loss 5.641, Val loss 6.604\n",
      "Ep 12 (Step 000105): Train loss 5.549, Val loss 6.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the, the the the the the the the the the the the, the the the the the the the\n",
      "Ep 13 (Step 000110): Train loss 5.470, Val loss 6.701\n",
      "Ep 13 (Step 000115): Train loss 5.399, Val loss 6.664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you the the the a a a a a, the the the the the the the the the the the the a the the the the the the the the the the the the the the the the the the, the, the the the the the the the\n",
      "Ep 14 (Step 000120): Train loss 5.261, Val loss 6.698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 14 (Step 000125): Train loss 5.260, Val loss 6.773\n",
      "Every effort moves you the the a a of the a a a--.                                       \n",
      "Ep 15 (Step 000130): Train loss 5.154, Val loss 6.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you that that that the the the the the the I had-- it had-- had-- it, I was.                            \n",
      "Ep 16 (Step 000135): Train loss 5.172, Val loss 6.747\n",
      "Ep 16 (Step 000140): Train loss 5.031, Val loss 6.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you I I I, I, I, the I, the the the the the the I, I, the the I, the I, the the I, I, I, I, the I, the to the I, the I, I\n",
      "Ep 17 (Step 000145): Train loss 5.059, Val loss 6.620\n",
      "Ep 17 (Step 000150): Train loss 4.958, Val loss 6.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you the the the the the the the the the the the the the the the the the the the the the the the, on the the the the the to the the the the the the the the on the the the the on the the the the the\n",
      "Ep 18 (Step 000155): Train loss 4.859, Val loss 6.784\n",
      "Ep 18 (Step 000160): Train loss 4.704, Val loss 6.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you a a a a-- a a to to a a a a to a to a------------ \"-- on a one, the a a a a a one-- on the one of the a a one of the a one of\n",
      "Ep 19 (Step 000165): Train loss 4.605, Val loss 6.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 19 (Step 000170): Train loss 4.504, Val loss 6.827\n",
      "Every effort moves you through a a to to a to to to to a to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to--. I had on to\n",
      "Ep 20 (Step 000175): Train loss 4.370, Val loss 6.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you through a a a-- a to a-- it. I had to to on to it. I was-- it was was-- it had to to it. I was was was was was was was-- it was was was was-- was was--\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(1)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), learning_rate=1e-4, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练和验证集的损失变化可视化，以便更直观地对比和分析模型的学习过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = ops.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从训练结果可以观察到以下几点：\n",
    "\n",
    "- 模型在训练初期生成的文本通常是难以理解的随机字符串，而随着训练进行，它逐渐能够生成语法或多或少正确的句子，表明模型正在学习语言的基本结构。\n",
    "\n",
    "- 然而，通过比较训练集和验证集的损失曲线，可以发现模型开始出现过拟合现象，即模型在训练集上的性能持续改善，但在验证集上的性能开始停滞或恶化。\n",
    "\n",
    "- 如果仔细检查模型在训练后期生成的一些段落，会发现它们几乎一字不差地出现在训练集中——这表明模型开始简单地记住训练数据，而不是学习泛化能力。\n",
    "\n",
    "- 在后续章节中，我们将介绍一些解码策略，这些策略能在一定程度上减轻这种记忆现象，增加生成文本的多样性。\n",
    "\n",
    "- 需要注意的是，这里的过拟合主要是因为我们的训练集非常小，而且我们对其进行了多次迭代。在实际的大规模LLM训练中，使用足够大的数据集和适当的正则化技术可以有效减轻过拟合问题。\n",
    "\n",
    "- 本章的LLM训练主要出于教育目的，让我们能够直观地理解模型学习过程；在实际应用中，通常需要更大的数据集和更复杂的训练策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 解码策略\n",
    "\n",
    "生成模型的解码策略是指从模型输出的概率分布中选择下一个词元的方法。不同的解码策略会导致生成文本具有不同的特性，如创造性、多样性和连贯性。下面，我们首先检查未训练模型的输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you through a a a-- a to a-- it. I had to to on to it. I was-- it was was\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，未训练的模型输出仍然是无意义的词元序列。接下来，我们将探索几种常用的解码策略，以改善生成文本的质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3.1 Temperature scaling\n",
    "\n",
    "Temperature scaling是LLM中用于控制生成文本多样性和随机性的关键技术。它通过调整softmax函数前的logits值来影响最终的概率分布。\n",
    "\n",
    "在之前的generate_text_simple函数中，我们使用torch.argmax（也称为贪婪解码）将概率最高的词元作为下一个词元。这种方法总是选择最可能的下一个词元，但可能导致生成的文本缺乏多样性，甚至陷入重复模式。\n",
    "\n",
    "首先，为了生成更多样化的文本，我们可以替换掉argmax，采用一种从概率分布中采样的方法。这允许模型有时选择概率较低的词元，增加生成文本的变化性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "vocab = { \n",
    "    \"closer\": 0,\n",
    "    \"every\": 1, \n",
    "    \"effort\": 2, \n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5, \n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8\n",
    "} \n",
    "\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Suppose input is \"every effort moves you\", and the LLM\n",
    "# returns the following logits for the next token:\n",
    "next_token_logits = Tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "softmax = nn.Softmax(axis=0)\n",
    "probas = softmax(next_token_logits)\n",
    "next_token_id = ops.argmax(probas)\n",
    "next_token_id = int(next_token_id.asnumpy())\n",
    "# The next generated token is then as follows:\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "next_token_id = ops.multinomial(probas, num_samples=1)\n",
    "next_token_id = int(next_token_id.asnumpy())\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多项式分布采样允许模型根据预测的概率分布随机选择下一个词元，这比贪婪解码能生成更多样化的文本。\n",
    "\n",
    "为便于说明，我们根据预测的概率进行1000次采样，观察不同词元被选中的频率："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.0 x closer\n",
      "1.0 x every\n",
      "0.0 x effort\n",
      "563.0 x forward\n",
      "4.0 x inches\n",
      "0.0 x moves\n",
      "0.0 x pizza\n",
      "364.0 x toward\n",
      "3.0 x you\n"
     ]
    }
   ],
   "source": [
    "def print_sampled_tokens(probas):\n",
    "    mindspore.set_seed(123) # Manual seed for reproducibility\n",
    "    sample = [int(ops.multinomial(probas, num_samples=1).asnumpy()) for i in range(1_000)]\n",
    "    sampled_ids = ops.bincount(Tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其次，为了进一步控制这一选择过程，我们引入temperature缩放这一概念。Temperature是一个正值参数，用于调整logits分布的\"尖锐度\"：\n",
    "\n",
    "当temperature=1时，概率分布保持不变\n",
    "\n",
    "当temperature<1时，概率分布变得更尖锐，高概率词元被选中的可能性增加，生成的文本更加确定性和一致性\n",
    "\n",
    "当temperature>1时，概率分布变得更平坦，不同词元被选中的可能性更加均衡，生成的文本更加多样和创造性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    softmax = nn.Softmax(axis=0)\n",
    "    return softmax(scaled_logits)\n",
    "\n",
    "# Temperature values\n",
    "temperatures = [1, 0.1, 5]  # Original, higher confidence, and lower confidence\n",
    "\n",
    "# Calculate scaled probabilities\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们可视化不同temperature值对概率分布的影响："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy\n",
    "# Plotting\n",
    "x = numpy.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    scaled_prob = scaled_probas[i].asnumpy()\n",
    "    rects = ax.bar(x + i * bar_width, scaled_prob, bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"temperature-plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以观察到，降低temperature使概率分布更加集中于高概率词元："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 x closer\n",
      "0.0 x every\n",
      "0.0 x effort\n",
      "990.0 x forward\n",
      "0.0 x inches\n",
      "0.0 x moves\n",
      "0.0 x pizza\n",
      "10.0 x toward\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而提高temperature则使概率分布更加均匀，增加了低概率词元被选中的可能性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148.0 x closer\n",
      "80.0 x every\n",
      "44.0 x effort\n",
      "256.0 x forward\n",
      "75.0 x inches\n",
      "44.0 x moves\n",
      "38.0 x pizza\n",
      "222.0 x toward\n",
      "93.0 x you\n"
     ]
    }
   ],
   "source": [
    "print_sampled_tokens(scaled_probas[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当temperature=5时，概率分布变得更加平坦，即使\"forward\"的原始概率远高于\"pizza\"，在采样时二者被选中的概率差距也大大减小。这种平衡可以帮助模型生成更加创新和多样化的文本，但太高的temperature可能导致生成的文本缺乏连贯性和意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Top-k 采样\n",
    "\n",
    "Top-k采样是另一种常用的解码策略，它通过限制模型在每个生成步骤中仅考虑概率最高的前k个候选词，从而平衡生成结果的多样性与合理性。这种方法避免了模型选择极低概率的词元，这些词元可能导致生成的文本语义不连贯。\n",
    "\n",
    "<img src=\"./images_llm/fig5.10.svg\" width='600'>\n",
    "\n",
    "Top-k方法通过将所有未选中的 `logit` 值替换为 `-inf`，使得在计算概率值时，非top-k词元的概率得分为 0，其余词元的概率总和保持为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们找出logits中前k个最大值及其位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: [6.75      6.28125   4.5117188]\n",
      "Top positions: [3 7 0]\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = ops.topk(next_token_logits, top_k)\n",
    "\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，将非top-k词元的logits设置为负无穷大，确保它们在softmax后的概率为0："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf -inf -inf 6.75 -inf -inf -inf 6.28 -inf]\n"
     ]
    }
   ],
   "source": [
    "new_logits = ops.where(\n",
    "    condition=next_token_logits < top_logits[-1],\n",
    "    x=Tensor(float(\"-inf\")), \n",
    "    y=next_token_logits\n",
    ")\n",
    "\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，应用softmax获得仅包含top-k词元的概率分布："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.6153837  0.         0.\n",
      " 0.         0.38461632 0.        ]\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(axis=0)\n",
    "topk_probas = softmax(new_logits)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 修改文本生成函数\n",
    "\n",
    "现在，我们将温度缩放和top-k采样相结合，对之前用于通过LLM生成文本的`generate_text_simple`函数进行改进，创建一个新的`generate`函数。这个新函数提供了更多的灵活性，允许调整temperature和top-k参数来控制生成文本的特性："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx = ops.cast(idx, mindspore.int32)\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = ops.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = ops.where(logits < min_val, Tensor(float(\"-inf\")), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            softmax = nn.Softmax(axis=-1)\n",
    "            probs = softmax(logits)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = ops.multinomial(probs, num_samples=1, replacement=False)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = ops.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "        # print(idx_next)\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = ops.cat((idx, idx_next), axis=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用新的`generate`函数，我们可以结合top-k采样和temperature调整来生成更加多样化和有意义的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you sun foris of so more, a put one the put put forburn\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成的文本显示，即使使用未经过充分训练的模型，通过适当的解码策略也能在一定程度上改善输出质量。在实际应用中，选择合适的解码参数对于平衡文本的创造性和连贯性至关重要：\n",
    "\n",
    "- 较低的temperature（如0.7）和适中的top-k值（如40）适合生成连贯、保守的回应\n",
    "\n",
    "- 较高的temperature（如1.2）和较大的top-k值（如100）适合生成创造性、多样化的内容\n",
    "\n",
    "- 在生成诗歌、故事等创意内容时，可以使用较高的temperature\n",
    "\n",
    "- 在生成技术文档、代码等精确内容时，可以使用较低的temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 加载和保存模型权重\n",
    "\n",
    "在实际应用中，保存和加载模型权重是必不可少的功能，它允许我们保存训练进度，并在需要时恢复模型状态。MindSpore提供了简便的API来实现这一功能。\n",
    "\n",
    "<img src=\"./images_llm/fig5.11.svg\" width='600'>\n",
    "\n",
    "\n",
    "保存模型的训练参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindspore.save_checkpoint(model, \"model_pretrained.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载保存的权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(98110:281473781230448,MainProcess):2025-03-13-19:39:28.332.488 [mindspore/nn/layer/basic.py:171] This parameter `dtype` will be deleted or invisible in the future. Please don't use it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel<\n",
       "  (tok_emb): Embedding<vocab_size=50257, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=tok_emb.embedding_table, shape=(50257, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (pos_emb): Embedding<vocab_size=256, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=pos_emb.embedding_table, shape=(256, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (drop_emb): Dropout<p=0.1>\n",
       "  (trf_blocks): SequentialCell<\n",
       "    (0): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (1): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (2): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (3): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (4): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (5): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (6): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (7): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (8): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (9): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (10): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (11): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    >\n",
       "  (final_norm): LayerNorm<>\n",
       "  (out_head): Dense<input_channels=768, output_channels=50257>\n",
       "  >"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "param_dict = mindspore.load_checkpoint(\"model_pretrained.ckpt\")\n",
    "param_not_load, _ = mindspore.load_param_into_net(model, param_dict)\n",
    "print(param_not_load) # param_not_load是未被加载的参数列表，为空时代表所有参数均加载成功。\n",
    "model.set_train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型权重的保存和加载不仅对于恢复中断的训练过程很重要，也是部署模型和分享预训练模型的基础。通过保存检查点，我们可以：\n",
    "\n",
    "1. 在训练过程中定期保存模型，防止因意外中断导致的训练进度丢失\n",
    "2. 选择表现最佳的模型版本进行部署\n",
    "3. 在不同的计算环境之间迁移模型\n",
    "4. 与其他研究者分享预训练模型，便于进一步的研究和应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 从 OpenAI 加载预训练权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了从头训练模型外，我们还可以利用已有的预训练模型权重。OpenAI发布了多个版本的GPT-2预训练权重，我们可以将这些权重加载到我们的模型架构中。\n",
    "\n",
    "首先，我们需要安装必要的依赖："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install  tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 gpt_download.py 文件中导入 download_and_load_gpt2 函数，如下所示。该函数将 GPT-2 架构设置 (settings) 和权重参数 (params) 加载到Python会话中："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative import from the gpt_download.py contained in this folder\n",
    "from gpt_download import download_and_load_gpt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们下载并加载GPT-2模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看加载的模型设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看参数字典的结构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "params字典存储LLM架构和权重信息。我们可以查看其中的一部分，例如词元嵌入权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们通过 `download_and_load_gpt2(model_size=\"124M\", ...)` 下载并加载了最小 GPT-2 模型的权重。OpenAI还提供了几种不同大小的预训练模型，每种模型具有不同的参数规模和能力：\n",
    "\n",
    "<img src=\"./images_llm/fig5.12.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们创建一个字典，列出图中不同 GPT 模型大小之间的差异："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些不同大小的模型在参数数量、嵌入维度、层数和注意力头数上有所不同。通常，参数量越大的模型表现越好，但计算需求也越高。\n",
    "\n",
    "假设加载最小的模型“`gpt2-small (124M)`”，可以使用 `model_configs` 表中的相应设置来更新 `GPT_CONFIG_124M`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(98110:281473781230448,MainProcess):2025-03-13-19:40:31.158.797 [mindspore/nn/layer/basic.py:171] This parameter `dtype` will be deleted or invisible in the future. Please don't use it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel<\n",
       "  (tok_emb): Embedding<vocab_size=50257, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=tok_emb.embedding_table, shape=(50257, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (pos_emb): Embedding<vocab_size=1024, embedding_size=768, use_one_hot=False, embedding_table=Parameter (name=pos_emb.embedding_table, shape=(1024, 768), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
       "  (drop_emb): Dropout<p=0.1>\n",
       "  (trf_blocks): SequentialCell<\n",
       "    (0): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (1): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (2): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (3): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (4): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (5): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (6): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (7): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (8): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (9): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (10): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    (11): TransformerBlock<\n",
       "      (att): MultiHeadAttention<\n",
       "        (W_query): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_key): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (W_value): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (out_proj): Dense<input_channels=768, output_channels=768, has_bias=True>\n",
       "        (dropout): Dropout<p=0.1>\n",
       "        >\n",
       "      (ff): FeedForward<\n",
       "        (layers): SequentialCell<\n",
       "          (0): Dense<input_channels=768, output_channels=3072, has_bias=True>\n",
       "          (1): GELU<>\n",
       "          (2): Dense<input_channels=3072, output_channels=768, has_bias=True>\n",
       "          >\n",
       "        >\n",
       "      (norm1): LayerNorm<>\n",
       "      (norm2): LayerNorm<>\n",
       "      (drop_shortcut): Dropout<p=0.1>\n",
       "      >\n",
       "    >\n",
       "  (final_norm): LayerNorm<>\n",
       "  (out_head): Dense<input_channels=768, output_channels=50257>\n",
       "  >"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])\n",
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.set_train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们定义一个辅助函数，用于检查和设置参数形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    print(f\"Shape. Left: {left.shape}, Right: {right.shape}\")\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return mindspore.Parameter(Tensor(right))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们定义一个 `load_weights_into_gpt` 函数，将 `params` 字典中的权重加载到 `GPTModel` 实例中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape. Left: (1024, 768), Right: (1024, 768)\n",
      "Shape. Left: (50257, 768), Right: (50257, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768, 768), Right: (768, 768)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (3072, 768), Right: (3072, 768)\n",
      "Shape. Left: (3072,), Right: (3072,)\n",
      "Shape. Left: (768, 3072), Right: (768, 3072)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (768,), Right: (768,)\n",
      "Shape. Left: (50257, 768), Right: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mindspore import Tensor\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.embedding_table = assign(gpt.pos_emb.embedding_table, params['wpe'])\n",
    "    gpt.tok_emb.embedding_table = assign(gpt.tok_emb.embedding_table, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "    \n",
    "    \n",
    "load_weights_into_gpt(gpt, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数将预训练的权重映射到我们的模型架构中，使得我们可以直接使用预训练模型的能力。这是一个重要的技术，因为从头训练一个大型语言模型需要大量的计算资源和数据。\n",
    "\n",
    "如果模型加载正确，我们可以使用它通过我们之前的 `generate` 函数生成新文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you as far as you can from one point to another, and each step means nothing except your progress.\n",
      "\n",
      "\n",
      "This post was\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(12)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.5\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，使用预训练权重的模型生成的文本比从头训练的模型更加连贯和有意义。这是因为预训练模型已经从大量文本数据中学习了语言的结构和模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 总结\n",
    "\n",
    "在本章中，我们实现了大语言模型的预训练过程，并探索了几种常用的文本生成解码策略。主要内容包括：\n",
    "\n",
    "1. 评估生成文本模型：我们学习了如何使用交叉熵损失和困惑度来评估语言模型的性能。\n",
    "2. 训练LLM：我们实现了一个简单的训练循环，用于在未标记文本数据上预训练语言模型。\n",
    "3. 解码策略：我们探讨了几种关键的解码策略，包括贪婪解码、温度缩放和top-k采样，它们对于控制生成文本的质量和多样性至关重要。\n",
    "4. 模型权重管理：我们学习了如何保存和加载模型权重，以及如何使用预训练的GPT-2模型权重。\n",
    "\n",
    "这些技术为构建和使用大语言模型提供了坚实的基础。在实际应用中，我们可以根据具体需求选择不同规模的预训练模型，并使用适当的解码策略来生成所需类型的文本。\n",
    "\n",
    "在下一章中，我们将探讨如何对预训练模型进行微调，使其适应特定的任务和领域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.15.0",
   "language": "python",
   "name": "tensorflow-1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
