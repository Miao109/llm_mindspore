{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  2.1.0\n",
      "The result of multiplication calculation is correct, MindSpore has been installed on platform [Ascend] successfully!\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "# mindspore.set_context(device_target='CPU')\n",
    "# mindspore.set_context(device_target='GPU')\n",
    "mindspore.set_context(device_target=\"Ascend\")\n",
    "mindspore.set_context(device_id=0)\n",
    "mindspore.run_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 处理文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在大语言模型（LLM）的开发过程中，数据准备是至关重要的一步。本章将详细介绍如何对文本数据进行处理，包括分词、词元化、词嵌入等关键技术，为后续的模型训练打下坚实基础。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images_llm/fig2.1.svg' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 理解词嵌入\n",
    "\n",
    "- 词嵌入（Word Embedding）是将词语映射到连续向量空间的技术，能够捕捉词语之间的语义关系。\n",
    "\n",
    "- 我们能够对各种数据类型进行嵌入处理，包括视频、音频以及文本。本书只关注文本数据的嵌入。\n",
    "\n",
    "<img src=\"./images_llm/fig2.2.svg\" width='600'>\n",
    "\n",
    "- 词嵌入能够捕捉词语之间的语义关系，使得语义相似的词在向量空间中的距离较近。\n",
    "\n",
    "- 词嵌入是LLM处理文本数据的基础，通过它，模型才能理解和生成人类语言。\n",
    "\n",
    "- 词向量可以拥有从一维到数千维的不同维度。下图展示了二维的情况。在实际应用中，词嵌入通常是高维的，以捕捉更丰富的语义信息。\n",
    "\n",
    "<img src=\"./images_llm/fig2.3.svg\" width='600'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 分词\n",
    "\n",
    "- 分词是将连续文本分解为离散单元的过程，这些离散单元称为词元(tokens)。\n",
    "\n",
    "- 对文本进行分词，即将文本分割成单独的词元，词元可以是单个单词、字符或子词单元。\n",
    "\n",
    "- 分词是将人类语言转换为机器可理解格式的第一步，也是后续处理的基础。\n",
    "\n",
    "- 不同的语言和应用场景可能需要不同的分词策略。\n",
    "\n",
    "<img src='./images_llm/fig2.4.svg' width='600'>\n",
    "\n",
    "- 加载原始文本，以短篇小说The Verdict为例。这是我们将要处理的示例文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/Miao109/\"\n",
    "           \"llm_mindspore/refs/heads/main/the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 本节的目标是将这个包含 20,479 个字符的短篇小说分解成单个单词和特殊字符，这样模型才能处理这些信息。\n",
    "\n",
    "- 开发一个简单的分词器：首先尝试根据空白字符拆分文本，这是最基本的分词方法之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "text = \"Hello, world. This, is a test.\" \n",
    "result = re.split(r'(\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种简单的分词方案可以将示例文本拆分为单个单词；但是，可以看到有一些单词仍然与标点符号连在一起，我们希望将单词与标点符号分开，使它们作为列表中的单独元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，上述分词结果中列表中还有一些空白字符，这些往往是不必要的。我们可以通过过滤方法移除这些空白字符，进一步净化分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item for item in result if item.strip()] \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了处理更复杂的文本，我们还需要对其他类型的标点符号进行同样的处理，比如问号、破折号等。下面的代码展示了如何处理更多种类的标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\" \n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) \n",
    "result = [item.strip() for item in result if item.strip()] \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images_llm/fig2.5.svg' width='600'>\n",
    "\n",
    "现在，让我们将这种分词方法应用到伊迪丝·华顿的整个短篇小说上，看看效果如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text) \n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()] \n",
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 用词元ID表示词元\n",
    "\n",
    "在完成分词后，下一步是将这些词元从字符串形式转换为整数形式，即生成词元ID。这是词元到嵌入向量转换过程的中间步骤，也是模型能够处理文本的关键环节。\n",
    "\n",
    "\n",
    "<img src='./images_llm/fig2.6.svg' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要构建一个包含所有词元并按字母顺序排列的词汇表。这个词汇表将作为从词元到整数ID的映射基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed)) \n",
    "vocab_size = len(all_words) \n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)} \n",
    "for i, item in enumerate(vocab.items()): \n",
    "    print(item) \n",
    "    if i >= 50: \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用现有的词汇表将新文本转换为词元ID。通过这种方式，我们可以将任意文本转换为模型可以处理的数字序列。\n",
    "\n",
    "<img src=\"./images_llm/fig2.7.svg\" width='600'>\n",
    "\n",
    "为了方便使用，我们将这些代码整合成一个分词器类，该类可以实现文本到词元ID的转换，以及从词元ID返回到文本的解码功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1: \n",
    "    def __init__(self, vocab): \n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} \n",
    "    def encode(self, text): \n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text) \n",
    "        preprocessed = [ \n",
    "            item.strip() for item in preprocessed if item.strip() \n",
    "        ] \n",
    "        ids = [self.str_to_int[s] for s in preprocessed] \n",
    "        return ids \n",
    "    def decode(self, ids): \n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`encode`函数将文本编码为词元ID序列，`decode`函数则将词元ID序列解码回文本。这两个函数是分词器的核心功能。\n",
    "\n",
    "<img src='./images_llm/fig2.8.svg' width='600'>\n",
    "\n",
    "现在，让我们使用SimpleTokenizerV1类实例化一个新的分词器对象，并对短篇小说\"the-verdict.txt\"中的一段文字进行分词处理，看看效果如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab) \n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "            Mrs. Gisburn said with pardonable pride.\"\"\" \n",
    "ids = tokenizer.encode(text) \n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将这些整数值转换回文本，验证我们的分词器是否能够正确地进行编码和解码操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 添加特殊上下文词元\n",
    "\n",
    "为了增强模型对文本中上下文或其他相关信息的理解，我们需要添加一些特殊上下文词元。这些特殊词元具有特定的功能和含义：\n",
    "\n",
    "- `<|unk|>`：用于表示未知单词，即词汇表中不存在的词。\n",
    "\n",
    "- `<|endoftext|>`：用于表示文档的结束或文本的边界，帮助模型识别不同文本之间的分隔。\n",
    "\n",
    "<img src='./images_llm/fig2.9.svg' width='600'>\n",
    "\n",
    "基于不同的LLM架构和应用需求，研究人员可能还会考虑其他一些特殊词元：\n",
    "- `[BOS]`（beginning of sequence）—— 用于表示文档的开始，以便模型能够识别输入何时开始。\n",
    "- `[EOS]`（end  of sequence）—— 用于标明输入序列的结束，在将多个不相关的文本连接起来时特别有用，这一点与 <|endoftext|> 标记类似。例如，在合并两篇不同的维基百科文章或书籍时，\n",
    "- `[EOS]` 标记表明一篇内容在哪里结束以及下一篇从哪里开始。> \n",
    "- `[PAD]`（padding）—— 当训练LLM时所用的批次大小大于 1时，批次中可能包含长度各异的文本。为确保所有文本长度相同，会使用 [PAD] 对较短的文本进行“填充”。\n",
    "\n",
    "在实际应用中，我们经常在不相关的文本之间添加<|endoftext|>特殊词元，以清晰地标记文本边界。\n",
    "\n",
    "<img src='./images_llm/fig2.10.svg' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们尝试将之前构建的分词器应用于一段新文本，看看会发生什么："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34572/1160838860.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_34572/3975630850.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         ] \n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_34572/3975630850.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         ] \n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\" \n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现当分词器遇到词汇表中不存在的单词（如\"Hello\"）时会抛出错误。这说明我们需要改进分词器，使其能够处理未知词。\n",
    "\n",
    "为此，我们将两个特殊词元`<|unk|>`和`<|endoftext|>`添加到词汇表中，用于处理这些特殊情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed))) \n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"]) \n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-5:]): \n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们对分词器进行相应的调整，使其能够正确处理未知词和文本边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2: \n",
    "    def __init__(self, vocab): \n",
    "        self.str_to_int = vocab \n",
    "        self.int_to_str = { i:s for s,i in vocab.items()} \n",
    "    def encode(self, text): \n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) \n",
    "        preprocessed = [ \n",
    "            item.strip() for item in preprocessed if item.strip() \n",
    "        ] \n",
    "        preprocessed = [item if item in self.str_to_int \n",
    "        else \"<|unk|>\" for item in preprocessed] \n",
    "        ids = [self.str_to_int[s] for s in preprocessed] \n",
    "        return ids \n",
    "    def decode(self, ids): \n",
    "        text = \" \".join([self.int_to_str[i] for i in ids]) \n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text) \n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在让我们使用改进后的分词器处理包含特殊词元的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\" \n",
    "text2 = \"In the sunlit terraces of the palace.\" \n",
    "text = \" <|endoftext|> \".join((text1, text2)) \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab) \n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 字节对编码\n",
    "\n",
    "现在我们来介绍一种更先进、更复杂的分词方案——字节对编码（BPE）。BPE最初是作为一种文本压缩算法开发的，后来被OpenAI在预训练GPT系列模型时广泛采用作为分词器的核心算法，包括GPT-2、GPT-3以及ChatGPT等。\n",
    "\n",
    "BPE的主要优势在于它能够有效地处理未登录词（OOV）问题，同时保持词汇表规模合理。它通过将常见的词保持完整，同时将罕见词分解为子词单元的方式实现这一点。\n",
    "\n",
    "由于BPE的实现过程相对复杂，我们将借助现有的Python开源库——\"[transformers](https://github.com/huggingface/transformers)\"或\"tiktoken\"来进行相关操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = ( \n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\" \n",
    "    \"of someunknownPlace.\" \n",
    ") \n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}) \n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers) \n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE分词器的一个重要特性是能够处理未知词。下面我们将看到它是如何处理含有未知词的文本的：\n",
    "\n",
    "<img src='./images_llm/fig2.11.svg' width=\"600\">\n",
    "\n",
    "与我们之前实现的简单分词器不同，基于BPE算法的分词器在处理未知单词时，不会简单地将其标记为`<|unk|>`，而是会将其分解为子词单元或字符序列，这使得模型能够对未见过的词汇保持一定程度的理解能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 数据采样：滑动窗口\n",
    "\n",
    "在准备好分词器后，下一步是为模型训练准备适当的数据格式。LLM主要通过预测文本中的下一个词元来进行预训练，因此我们需要将文本数据组织成一系列的输入-目标对，其中目标是输入序列中的下一个词元。\n",
    "\n",
    "滑动窗口是一种常用的数据采样方法，它通过在文本上滑动固定大小的窗口来创建这些输入-目标对。\n",
    "\n",
    "<img src=\"./images_llm/fig2.12.svg\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们使用分词器对整个文本进行编码，得到词元ID序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过滑动窗口方法，我们可以从预处理后的文本中提取大量样本，得到一系列的输入-目标对。由于语言模型的核心任务是预测下一个词元，所以目标是输入向右偏移一个位置后对应的词元。\n",
    "\n",
    "下面演示如何使用滑动窗口创建简单的输入-目标对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y:      [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更清楚地理解输入和目标之间的关系，我们可以将词元ID解码为实际文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1): \n",
    "    context = enc_sample[:i] \n",
    "    desired = enc_sample[i] \n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型训练之前，我们需要一个高效的数据加载器来批量处理这些输入-目标对。这个数据加载器将遍历数据集，并以MindSpore张量的形式返回批量的输入数据和目标数据。\n",
    "\n",
    "<img src=\"./images_llm/fig2.13.svg\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码实现了一个专门用于GPT模型的数据加载器，它使用滑动窗口方法进行数据采样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.dataset import GeneratorDataset\n",
    "from mindspore import Tensor\n",
    "class GPTDatasetV1:\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(Tensor(input_chunk))\n",
    "            self.target_ids.append(Tensor(target_chunk))\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，创建一个辅助函数来初始化和配置数据加载器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "    # 指定本地词汇表文件所在的目录路径\n",
    "    local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "    # 从本地路径加载GPT - 2分词器\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(local_path)\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = GeneratorDataset(dataset, [\"input_ids\", \"target_ids\"], shuffle=shuffle)\n",
    "    dataloader = dataloader.batch(batch_size, drop_remainder=drop_last)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设定batch大小为1，上下位大小为4，来测试我们的数据加载器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: \n",
    "    raw_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[1, 4], dtype=Int64, value=\n",
      "[[  40,  367, 2885, 1464]]), Tensor(shape=[1, 4], dtype=Int64, value=\n",
      "[[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1( \n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False) \n",
    "data_iter = iter(dataloader) \n",
    "first_batch = next(data_iter) \n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[1, 4], dtype=Int64, value=\n",
      "[[ 367, 2885, 1464, 1807]]), Tensor(shape=[1, 4], dtype=Int64, value=\n",
      "[[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter) \n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，stride的值决定了输入在不同批次间偏移的位置量，从而实现了滑动窗口方法。较小的stride值会产生更多的重叠样本，而较大的stride值则会减少重叠，但也会减少样本总数。\n",
    "\n",
    "<img src=\"./images_llm/fig2.14.svg\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[   40   367  2885  1464]\n",
      " [ 1807  3619   402   271]\n",
      " [10899  2138   257  7026]\n",
      " [15632   438  2016   257]\n",
      " [  922  5891  1576   438]\n",
      " [  568   340   373   645]\n",
      " [ 1049  5975   284   502]\n",
      " [  284  3285   326    11]]\n",
      "\n",
      "Targets:\n",
      " [[  367  2885  1464  1807]\n",
      " [ 3619   402   271 10899]\n",
      " [ 2138   257  7026 15632]\n",
      " [  438  2016   257   922]\n",
      " [ 5891  1576   438   568]\n",
      " [  340   373   645  1049]\n",
      " [ 5975   284   502   284]\n",
      " [ 3285   326    11   287]]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 词嵌入\n",
    "\n",
    "在完成分词和数据采样后，下一步是将词元ID转换为嵌入向量。词嵌入是一种将离散符号（如词元ID）映射到连续向量空间的技术，这使得模型能够捕捉词元之间的语义关系。\n",
    "\n",
    "<img src='./images_llm/fig2.15.svg' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用MindSpore的nn.Embedding层来实现词嵌入。假设有以下4个词元ID：2、3、5、1，我们将演示如何将它们转换为嵌入向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = Tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为简单起见，假设我们的词汇表仅包含6个单词，并设定嵌入向量的维度为3。在实际应用中，词汇表通常有上万甚至数十万个词元，嵌入维度也会更高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "from mindspore import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "mindspore.set_seed(123)\n",
    "embedding_layer = nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter (name=embedding_table, shape=(6, 3), dtype=Float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.embedding_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01257949, -0.00093973,  0.01836986],\n",
       "       [-0.00773198,  0.00244677, -0.00437232],\n",
       "       [-0.00352256,  0.00886481,  0.00904443],\n",
       "       [ 0.0110456 ,  0.00728295, -0.00120312],\n",
       "       [ 0.01376666,  0.0001967 , -0.00284691],\n",
       "       [ 0.00700223, -0.0051715 , -0.00239952]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.embedding_table.asnumpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层的权重矩阵最初是由随机值构成的。在LLM训练过程中，这些权重会作为模型参数进行优化，最终学习到有意义的语义表示。\n",
    "\n",
    "给定词元ID为3，我们可以使用嵌入层将其转换为对应的嵌入向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0110456   0.00728295 -0.00120312]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(Tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将嵌入层应用到一个词元ID序列上，得到一个嵌入向量序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00352256  0.00886481  0.00904443]\n",
      " [ 0.0110456   0.00728295 -0.00120312]\n",
      " [ 0.00700223 -0.0051715  -0.00239952]\n",
      " [-0.00773198  0.00244677 -0.00437232]]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嵌入层实质上是一种查找操作：它根据词元ID从嵌入矩阵中检索对应的向量行。这个过程非常高效，是深度学习中处理文本数据的基础操作。\n",
    "\n",
    "<img src=\"./images_llm/fig2.16.svg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 单词位置编码\n",
    "\n",
    "在上一节中，我们介绍了如何将词元转换为嵌入向量，但这些嵌入向量不包含词元在序列中的位置信息。在处理序列数据时，词元的位置往往包含重要的语义信息，例如\"猫追狗\"和\"狗追猫\"包含相同的词元但意思完全不同。\n",
    "\n",
    "为了解决这个问题，我们需要将位置信息融入到嵌入向量中，这就是位置编码的作用。\n",
    "\n",
    "<img src=\"./images_llm/fig2.17.svg\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置编码有多种实现方式，包括绝对位置编码和相对位置编码。在此，我们使用一种简单的可学习的绝对位置编码方法，即为每个位置创建一个嵌入向量，然后将其加到词元嵌入上。\n",
    "\n",
    "<img src=\"./images_llm/fig2.18.svg\"  width=\"600\">\n",
    "\n",
    "\n",
    "假定嵌入向量大小为256，另外假设词汇表大小为50257（这是GPT-2模型的词汇表大小）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们使用之前的数据加载器加载采样数据，并将其转换为嵌入向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'allowed_special': {'<|endoftext|>'}} not recognized.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5145 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " [[   40   367  2885  1464]\n",
      " [ 1807  3619   402   271]\n",
      " [10899  2138   257  7026]\n",
      " [15632   438  2016   257]\n",
      " [  922  5891  1576   438]\n",
      " [  568   340   373   645]\n",
      " [ 1049  5975   284   502]\n",
      " [  284  3285   326    11]]\n",
      "\n",
      "Inputs shape:\n",
      " (8, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs) \n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们创建一个位置嵌入层，用于生成与序列位置对应的嵌入向量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.numpy as mnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 256)\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(mnp.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将词元嵌入和位置嵌入相加，得到最终的输入表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4, 256)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的输入处理流程如下：输入文本被分解为单个词元；使用词汇表将这些词元转换为词元ID；词元ID被转换为嵌入向量；位置嵌入被加入到词元嵌入中，形成最终的输入表示。\n",
    "\n",
    "\n",
    "<img src=\"./images_llm/fig2.19.svg\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 总结\n",
    "\n",
    "在本章中，我们介绍了LLM处理文本数据的基本流程和关键技术：\n",
    "\n",
    "- **文本表示转换**：LLM无法直接处理原始文本，需要将其转换为数值向量（嵌入）。这个过程包括分词、词元ID生成和嵌入向量转换。\n",
    "\n",
    "- **分词技术**：我们从简单的基于规则的分词器开始，逐步引入了更复杂的BPE分词算法。BPE能够更好地处理未登录词，是现代LLM中广泛使用的分词方法。\n",
    "\n",
    "- **特殊词元**：为了增强模型的处理能力，我们引入了特殊词元如`<|unk|>`和`<|endoftext|>`，用于处理未知词和标记文本边界。\n",
    "\n",
    "- **数据采样**：通过滑动窗口方法，我们将连续文本转换为输入-目标对，适合LLM的自回归训练方式。\n",
    "\n",
    "- **词嵌入**：使用嵌入层将离散的词元ID转换为连续的向量表示，这些向量可以捕捉词元之间的语义关系。\n",
    "\n",
    "- **位置编码**：为了让模型理解序列中词元的位置信息，我们引入了位置编码技术，包括可学习的绝对位置编码。\n",
    "\n",
    "这些技术共同构成了LLM文本处理的基础架构，为后续的模型训练和推理奠定了基础。在下一章中，我们将开始探索如何构建和训练LLM模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.15.0",
   "language": "python",
   "name": "tensorflow-1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
