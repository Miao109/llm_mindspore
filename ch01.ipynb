{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 理解大型语言模型\n",
    "\n",
    "本教程深入探讨大型语言模型(LLMs)的本质、架构、训练方法及应用。通过系统化的学习，我们将揭开这些强大AI系统背后的原理，并理解它们如何改变自然语言处理领域。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 大型语言模型的历史与发展\n",
    "\n",
    "大型语言模型(LLMs)代表了自然语言处理（NLP）领域的范式转变。它们开创了一个新时代，使AI系统能够以前所未有的方式理解和生成人类语言。\n",
    "\n",
    "### 1.1.1 从规则到统计，再到深度学习\n",
    "\n",
    "NLP的发展经历了三个主要阶段：\n",
    "\n",
    "1. **规则时代（1950s-1980s）**：早期NLP系统依赖于语言学家手工编写的规则。这些系统在非常特定的任务上表现良好，但缺乏灵活性，无法应对语言的多样性和复杂性。\n",
    "\n",
    "2. **统计时代（1990s-2010s）**：研究人员开始使用统计方法从数据中学习模式。这包括n-gram模型、隐马尔可夫模型和条件随机场等。这些方法在特定任务如垃圾邮件过滤和简单分类上表现良好，但在需要深层理解的任务上仍显不足。\n",
    "\n",
    "3. **深度学习时代（2010s-至今）**：神经网络，特别是深度学习模型，彻底改变了NLP领域。从词嵌入（Word2Vec, GloVe）到循环神经网络（LSTM, GRU），再到现在的Transformer架构，我们见证了NLP能力的飞跃式提升。\n",
    "\n",
    "### 1.1.2 大型语言模型的出现\n",
    "\n",
    "现代意义上的LLMs始于2018年左右，当时Google发布了BERT，OpenAI发布了GPT系列的早期版本。这些模型标志着从特定任务模型向通用语言理解系统的转变。之后，随着GPT-3（2020年）、ChatGPT（2022年）等模型的发布，LLMs的规模和能力呈指数级增长。\n",
    "\n",
    "这些模型最令人惊叹的特点在于它们表现出的涌现能力（emergent abilities）—随着模型规模增大，它们不仅在已知任务上表现更好，还开始展示训练过程中未明确教授的新能力，如复杂推理、代码生成和多步骤问题解决等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 什么是大型语言模型？\n",
    "\n",
    "大型语言模型(LLMs)是设计用于理解、生成和回应类人文本的深度神经网络。当我们说LLMs能够理解语言时，意味着它们能以连贯且上下文相关的方式处理和生成文本，而非具有人类般的意识或理解能力。\n",
    "\n",
    "<img src='./images_llm/fig1.1.svg' width=\"600\">\n",
    "\n",
    "### 1.2.1 LLMs的核心特征\n",
    "\n",
    "- **规模巨大**：大型指的是模型参数数量和训练数据规模。现代LLMs通常拥有数十亿至数千亿个参数，训练数据可达数万亿个标记（tokens）。\n",
    "\n",
    "- **语言建模**：它们基于预测下一个词的任务进行训练，这种看似简单的任务实际上需要模型理解上下文、语法、语义，甚至世界知识。\n",
    "\n",
    "- **深度神经网络架构**：现代LLMs基于Transformer架构，这种架构允许模型高效处理长序列并捕捉远距离依赖关系。\n",
    "\n",
    "- **自监督学习**：LLMs主要通过自监督学习训练，意味着它们不需要人类显式标注的数据就能从原始文本中学习。\n",
    "\n",
    "### 1.2.2 与传统NLP方法的区别\n",
    "\n",
    "传统NLP系统与现代LLMs有几个关键区别：\n",
    "\n",
    "1. **任务特定 vs 通用**：传统NLP模型通常为特定任务设计，如情感分析或命名实体识别。相比之下，LLMs是通用系统，可以适应各种语言任务。\n",
    "\n",
    "2. **有限上下文 vs 长距离理解**：早期模型往往难以处理长距离依赖，而LLMs能够理解和维持更长的上下文。\n",
    "\n",
    "3. **特征工程 vs 端到端学习**：传统方法依赖专家定义的特征，而LLMs通过端到端学习自动发现有用的特征和模式。\n",
    "\n",
    "4. **预定义任务 vs 涌现能力**：LLMs展示了涌现能力，能够执行它们未被明确训练过的任务。\n",
    "\n",
    "### 1.2.3 LLMs的限制\n",
    "\n",
    "尽管功能强大，LLMs仍有重要限制：\n",
    "\n",
    "- **缺乏真正理解**：它们模仿理解，但不具备人类的概念理解或意识。\n",
    "- **幻觉问题**：可能生成看似合理但实际不正确的信息。\n",
    "- **训练数据偏见**：模型可能反映和放大训练数据中的偏见。\n",
    "- **计算需求巨大**：训练和运行大型模型需要显著的计算资源。\n",
    "- **时间限制**：知识截止于训练数据的收集时间，无法获取更新的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 LLMs的应用领域\n",
    "\n",
    "LLMs的能力使其在众多领域找到应用，从日常助手到专业工具。\n",
    "\n",
    "### 1.3.1 通用应用\n",
    "\n",
    "- **对话系统与虚拟助手**：如ChatGPT、Google的Gemini、Microsoft的Copilot等，能够进行自然对话并回答各种查询。\n",
    "\n",
    "- **内容创作**：协助撰写文章、营销文案、创意写作、诗歌和脚本等。\n",
    "\n",
    "- **教育工具**：提供个性化学习体验，解答学生问题，生成教育内容。\n",
    "\n",
    "- **编程助手**：生成代码，解释技术概念，调试问题，提供开发建议。\n",
    "\n",
    "### 1.3.2 专业领域应用\n",
    "\n",
    "- **医疗保健**：协助医学研究文献综述，辅助诊断，简化医疗记录，甚至协助药物发现过程。\n",
    "\n",
    "- **法律**：分析法律文件，生成标准合同，搜索判例法，简化法律研究。\n",
    "\n",
    "- **金融**：分析市场报告，生成财务摘要，识别趋势，风险评估。\n",
    "\n",
    "- **科学研究**：帮助分析科学文献，提出研究假设，生成实验设计，解释复杂数据。\n",
    "\n",
    "### 1.3.3 语言相关任务\n",
    "\n",
    "- **翻译**：提供比传统系统更自然、更上下文相关的多语言翻译。\n",
    "\n",
    "- **摘要**：生成长文档的精确摘要，保留关键信息。\n",
    "\n",
    "- **问答系统**：从大量信息中提取特定答案，支持复杂查询。\n",
    "\n",
    "- **情感分析**：分析文本情感，识别细微情绪差异。\n",
    "\n",
    "LLMs的应用几乎无限，我们仍在发现这些模型的新用途。重要的是，随着技术发展，LLMs正逐渐从通用助手转变为专业工具，在特定领域获得深度专业知识。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 LLM 的构建阶段\n",
    "\n",
    "构建大语言模型通常包括两个主要阶段：**预训练**和**微调**。\n",
    "\n",
    "### 预训练阶段\n",
    "- 在大规模、多样化的原生文本数据集上进行训练\n",
    "- 学习预测文本中的下一个单词\n",
    "- 形成对语言的广泛理解\n",
    "- 得到一个初始的\"基础模型\"\n",
    "\n",
    "### 微调阶段\n",
    "- 在更小、更有针对性的标记数据集上进行训练\n",
    "- 常见的微调方式：\n",
    "  - **指令微调**：使用由指令和答案对组成的数据集\n",
    "  - **分类微调**：使用由文本和相关类标签组成的数据集\n",
    "\n",
    "<img src='./images_llm/fig1.2.svg' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Transformer架构深度解析\n",
    "\n",
    "Transformer架构是现代LLMs的基础，它由Vaswani等人在2017年的论文“Attention Is All You Need”中首次提出。这种架构以其并行处理能力和捕获长距离依赖关系的能力彻底改变了NLP领域。\n",
    "\n",
    "\n",
    "### 1.5.1 传统序列模型的局限\n",
    "\n",
    "在Transformer出现前，NLP主要依赖递归神经网络(RNN)，如LSTM和GRU。这些模型存在几个关键限制：\n",
    "\n",
    "- **顺序处理**：必须按顺序处理输入，无法并行化，导致训练速度慢。\n",
    "- **梯度问题**：长序列上容易出现梯度消失/爆炸问题。\n",
    "- **有限记忆**：难以捕获远距离依赖关系。\n",
    "\n",
    "### 1.5.2 Transformer的核心组件\n",
    "\n",
    "#### 自注意力机制（Self-Attention）\n",
    "\n",
    "自注意力是Transformer的核心创新，允许模型评估输入序列内单词之间的关系。对每个输入位置，模型计算：\n",
    "\n",
    "- **查询（Query）**：当前标记想要了解什么。\n",
    "- **键（Key）**：其他标记提供的信息。\n",
    "- **值（Value）**：实际内容。\n",
    "\n",
    "通过计算查询与所有键的相似度，然后用这些相似度对值进行加权求和，模型可以动态关注相关上下文。这使得Transformer能够处理长距离依赖，例如识别代词引用的名词，即使它们相隔很远。\n",
    "\n",
    "#### 多头注意力（Multi-Head Attention）\n",
    "\n",
    "多头注意力允许模型同时关注不同位置的不同表示子空间，从而捕获更丰富的信息：\n",
    "\n",
    "- 一个头可能关注语法关系\n",
    "- 另一个头可能关注语义相似性\n",
    "- 第三个头可能跟踪主题连贯性\n",
    "\n",
    "#### 前馈神经网络（Feed-Forward Networks）\n",
    "\n",
    "每个注意力层后跟随一个前馈网络，应用于每个位置独立且相同的方式：\n",
    "\n",
    "- 通常由两个线性变换组成，中间有ReLU激活函数\n",
    "- 增加模型的表达能力，处理注意力机制捕获的模式\n",
    "\n",
    "#### 位置编码（Positional Encoding）\n",
    "\n",
    "由于Transformer没有固有的序列概念，需要位置编码来注入标记顺序信息：\n",
    "\n",
    "- 使用正弦和余弦函数生成唯一位置信号\n",
    "- 允许模型学习相对位置和绝对位置\n",
    "\n",
    "#### 残差连接和层归一化（Residual Connections & Layer Normalization）\n",
    "\n",
    "这些技术对稳定训练至关重要：\n",
    "\n",
    "- 残差连接帮助梯度流动，允许训练更深的网络\n",
    "- 层归一化稳定激活值，加速训练收敛\n",
    "\n",
    "### 1.5.3 编码器-解码器架构\n",
    "\n",
    "原始Transformer有两个主要组件：\n",
    "\n",
    "- **编码器**：对输入文本进行处理，将其编码为数字表征形式(向量)\n",
    "- **解码器**：接收已编码的向量，并据此生成输出文本\n",
    "\n",
    "<img src=\"./images_llm/fig1.3.svg\" width=\"600\">\n",
    "\n",
    "这种架构特别适合机器翻译等任务，其中输入和输出是不同的序列。然而，现代LLMs通常只使用架构的一部分，我们将在下一节讨论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 BERT与GPT：Transformer架构的两种路径\n",
    "\n",
    "基于Transformer的LLMs主要发展出两种不同的架构路径：以BERT为代表的编码器模型和以GPT为代表的解码器模型。\n",
    "\n",
    "<img src=\"./images_llm/fig1.4.svg\" width=\"600\">\n",
    "\n",
    "### 1.6.1 BERT：双向编码器表示\n",
    "\n",
    "BERT（Bidirectional Encoder Representations from Transformers）由Google于2018年发布，专注于Transformer的编码器部分。\n",
    "\n",
    "#### BERT的关键特性\n",
    "\n",
    "- **双向上下文**：BERT同时考虑左右两侧的上下文，使用完全没有方向性的注意力。\n",
    "  \n",
    "- **掩码语言建模（MLM）**：预训练过程中，随机掩盖15%的标记，让模型预测这些被掩盖的标记。这迫使模型利用双向上下文理解语言。\n",
    "  \n",
    "- **下一句预测（NSP）**：BERT也预测两个给定句子是否顺序相邻，帮助模型理解句子间关系。\n",
    "  \n",
    "- **静态表示**：BERT生成固定的上下文表示，非常适合分类和理解任务。\n",
    "\n",
    "#### BERT的应用领域\n",
    "\n",
    "BERT特别擅长：\n",
    "- 文本分类（情感分析、垃圾邮件检测）\n",
    "- 命名实体识别\n",
    "- 问答系统\n",
    "- 文本相似度计算\n",
    "\n",
    "#### BERT的局限性\n",
    "\n",
    "- 不太适合生成任务，因为它不是自回归模型\n",
    "- 处理长文本能力有限（原始BERT限制在512个标记）\n",
    "- 需要为不同任务进行显式微调\n",
    "\n",
    "### 1.6.2 GPT：生成式预训练转换器\n",
    "\n",
    "GPT（Generative Pretrained Transformer）系列由OpenAI开发，专注于Transformer的解码器部分。\n",
    "\n",
    "#### GPT的关键特性\n",
    "\n",
    "- **单向上下文**：GPT仅使用左侧（之前的）上下文，通过因果注意力掩码实现，因此每个标记只能关注其前面的标记。\n",
    "  \n",
    "- **自回归生成**：GPT预测序列中的下一个标记，一次一个，适合文本生成。\n",
    "  \n",
    "- **更大规模**： GPT系列持续增加参数量和训练数据规模（GPT-1：117M，GPT-2：1.5B，GPT-3：175B，GPT-4：估计超过1T）。\n",
    "  \n",
    "- **在下文预测任务上的涌现能力**：随着规模增加，GPT模型开始展示未明确训练的能力。\n",
    "\n",
    "GPT模型预测下一个词的示意图:\n",
    "\n",
    "<img src=\"./images_llm/fig1.6.svg\" width='500'>\n",
    "\n",
    "GPT 架构仅采用原始变压器的解码器部分:\n",
    "\n",
    "<img src=\"./images_llm/fig1.7.svg\" width='500'>\n",
    "\n",
    "\n",
    "#### GPT的应用领域\n",
    "\n",
    "GPT特别擅长：\n",
    "- 文本生成（创意写作、内容创作）\n",
    "- 对话系统\n",
    "- 代码生成\n",
    "- 文本摘要和扩展\n",
    "- 通过少样本学习解决多种任务\n",
    "\n",
    "<img src=\"./images_llm/fig1.5.svg\" width='600'>\n",
    "\n",
    "#### GPT的局限性\n",
    "\n",
    "- 幻觉问题（生成看似真实但实际不正确的信息）\n",
    "- 难以理解全局上下文（后面的内容不会影响对前面内容的理解）\n",
    "- 训练和推理的计算成本高昂\n",
    "\n",
    "### 1.6.3 GPT-3的训练数据\n",
    "\n",
    "GPT-3是在庞大且多样化的数据集上训练的，包括：\n",
    "\n",
    "| 数据集名称 | 数据集描述 | 词元数量 | 训练数据中的占比 |\n",
    "|------------|------------|----------|------------------|\n",
    "| CommonCrawl(filtered) | Web爬取数据 | 4100亿 | 60% |\n",
    "| WebText2 | Web爬取数据 | 190亿 | 22% |\n",
    "| Books1 | 基于互联网的书籍语料库 | 120亿 | 8% |\n",
    "| Books2 | 基于互联网的书籍语料库 | 550亿 | 8% |\n",
    "| Wikipedia | 高质量文本 | 30亿 | 3% |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 为什么要构建自己的LLM？\n",
    "\n",
    "从零开始构建LLM有以下优势：\n",
    "\n",
    "- 深入理解LLM的**运行机制与局限性**\n",
    "- 掌握对现有开源LLM架构进行**预训练或微调**的知识\n",
    "- 针对特定领域的数据集或任务进行**优化**\n",
    "- 保障**数据隐私**，避免与第三方LLM供应商共享敏感数据\n",
    "- 可直接部署于客户设备，降低**延迟**，减少服务器相关成本\n",
    "- 提供充分的**自主权**，允许按照实际需求对模型进行更新或修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 构建大型语言模型\n",
    "\n",
    "现在我们将深入探讨如何系统地构建一个大型语言模型。这个过程可以分为三个关键阶段，每个阶段都有其独特的挑战和方法论。\n",
    "\n",
    "### 1.8.1 从理论到实践的三阶段方法\n",
    "\n",
    "构建LLM是一个渐进的过程，正如图1.9所示，包含三个主要阶段：\n",
    "\n",
    "1. **架构实现与数据准备阶段**：在这个基础阶段，我们实现LLM的核心架构组件并准备训练数据。这包括数据采样策略的设计、自注意力机制的编程实现以及整体架构的构建。\n",
    "\n",
    "2. **预训练阶段**：这个阶段将模型暴露于海量未标记文本数据，使其通过预测下一个词来学习语言的基本模式和结构。预训练创建一个具有基本文本完成和少样本学习能力的通用基础模型。\n",
    "\n",
    "3. **微调阶段**：最后一个阶段将预训练模型针对特定任务进行专门化处理。这可以是将模型微调为分类器（如情感分析），或微调为个人助手（能够理解和遵循复杂指令）。\n",
    "\n",
    "### 1.8.2 实现挑战与实用方法\n",
    "\n",
    "构建商业级LLM面临显著挑战，尤其是在计算资源方面：\n",
    "\n",
    "- **计算成本**：从头预训练一个GPT类模型可能需要数千到数百万美元的计算资源。例如，GPT-3的预训练成本估计约为460万美元，使用了数千个GPU并耗时数月。\n",
    "\n",
    "- **数据规模**：预训练数据通常包含数十亿到数万亿个标记（词语和标点符号），需要数百GB到数TB的存储空间。\n",
    "\n",
    "- **工程复杂性**：分布式训练、数值稳定性、优化器调整等技术挑战需要专业知识。\n",
    "\n",
    "考虑到这些挑战，一个更实用的教育性方法是：\n",
    "\n",
    "1. **专注于核心组件实现**：深入理解和实现自注意力机制、位置编码等基础构建块。\n",
    "\n",
    "2. **使用小型数据集进行概念验证**：在有限数据上进行预训练演示，展示核心原理而非追求商业级性能。\n",
    "\n",
    "3. **利用开源预训练权重**：学习如何加载和使用现有的开源模型权重，避免昂贵的预训练阶段。\n",
    "\n",
    "4. **关注微调技术**：探索如何有效地微调预训练模型以适应特定任务和应用场景。\n",
    "\n",
    "### 1.8.3 构建过程的阶段性目标\n",
    "\n",
    "从零开始编写一个LLM可以分为三个主要阶段：\n",
    "\n",
    "1. **第一阶段**：实现LLM架构与数据准备流程\n",
    "   - 学习基本的数据预处理步骤\n",
    "   - 通过代码实现注意力机制\n",
    "\n",
    "2. **第二阶段**：对LLM进行预训练以生成基础模型\n",
    "   - 编写预训练代码\n",
    "   - 使用小数据集进行教学性质的训练\n",
    "   - 学习如何加载公开可用的模型权重\n",
    "\n",
    "3. **第三阶段**：对基础模型进行微调\n",
    "   - 使模型能够遵循指令\n",
    "   - 实现文本分类等任务\n",
    "\n",
    "<img src=\"./images_llm/fig1.8.svg\" width=\"600\">\n",
    "\n",
    "构建LLM是一个令人兴奋但具有挑战性的旅程。通过系统化的方法，我们可以深入理解这些强大系统背后的原理，并可能在此基础上创新和改进。无论是为了教育目的还是研究实验，构建即使是小型的LLM也能提供宝贵的见解和实践经验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 总结\n",
    "\n",
    "大语言模型(LLM)已经在自然语言处理领域引发了变革，为人类语言的理解、生成及翻译带来了显著进步。LLM的训练包含预训练和微调两个主要阶段，基于Transformer架构，特别是其核心的注意力机制。尽管预训练需要庞大的数据集和计算资源，但通过微调可以使模型适应特定任务，在各种应用场景中发挥强大作用。\n",
    "\n",
    "通过逐步构建的过程，我们可以深入了解像ChatGPT这样的复杂LLM助手的工作机制，并将这些知识应用到实际问题解决中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
