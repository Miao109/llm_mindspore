{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindSpore version:  2.1.0\n",
      "The result of multiplication calculation is correct, MindSpore has been installed on platform [CPU] successfully!\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "mindspore.set_context(device_target='CPU')\n",
    "# mindspore.set_context(device_target='GPU')\n",
    "# mindspore.set_context(device_target=\"Ascend\")\n",
    "# mindspore.set_context(device_id=0)\n",
    "mindspore.run_check()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 从头开始实现 GPT 模型来生成文本\n",
    "\n",
    "在前几章中，我们已经详细学习了文本处理的基础和注意力机制的原理与实现。现在，我们将这些知识整合起来，构建一个完整的生成式预训练Transformer (GPT)模型。GPT模型是现代大语言模型的基础架构，通过掌握它的实现原理，我们可以深入理解当前最先进的语言模型是如何工作的。\n",
    "\n",
    "本章将从零开始逐步构建GPT模型的各个组件，并将它们组装成一个能够生成连贯文本的完整系统。我们将着手编写LLM的核心构建模块，如层归一化、前馈网络和Transformer块，最终实现一个小型但功能完整的GPT模型。\n",
    "\n",
    "<img src=\"./images_llm/fig4.1.svg\" width='600'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 编写 LLM 架构\n",
    "\n",
    "大语言模型(LLMs)如GPT-3、LLaMA和PaLM通常包含数十亿甚至数千亿参数，这使它们在人类眼中显得极其庞大和复杂。然而，尽管这些模型的规模令人生畏，但其基础架构相对简单且高度重复。它们主要由相同的Transformer块堆叠而成，每个块负责处理不同抽象层次的信息。\n",
    "\n",
    "这种模块化设计使得LLM可以通过简单地增加这些基本构建块的数量来扩展规模，而不需要从根本上改变其架构。正是这种可扩展性使得模型能够从最初的数百万参数扩展到现在的数千亿参数。\n",
    "    \n",
    "\n",
    "<img src='./images_llm/fig4.2.svg' width='400'>\n",
    "\n",
    "我们将编写最小的 GPT-2 模型（1.24 亿参数）的架构，通过以下 Python 字典指定模型的配置。这种配置驱动的设计方法具有很强的灵活性，我们只需调整这些参数值，就可以实现不同规模和能力的GPT模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 GPT_CONFIG_124M 词典中，以下是对一些变量名的详细解释：\n",
    "\n",
    "1. **vocab_size** (50257): 词汇表大小，指的是BPE分词器使用的词汇量。这个特定的数字来自于GPT-2原始实现，包含了常见英文单词、子词单元和特殊标记。\n",
    "\n",
    "2. **context_length** (1024): 模型可以处理的最大序列长度。这决定了模型一次能够\"看到\"和处理的文本量，对于理解长文本和维持上下文连贯性至关重要。\n",
    "\n",
    "3. **emb_dim** (768): 嵌入维度，每个词元转换为多维向量表示的维度。较高的维度允许模型捕捉更丰富的语义信息，但也增加了计算成本。\n",
    "\n",
    "4. **n_heads** (12): 多头注意力机制中的注意力头数量。多头设计使模型能够同时关注序列中的不同信息模式，从而增强模型的表达能力。\n",
    "\n",
    "5. **n_layers** (12): 模型中堆叠的Transformer块数量。更多的层可以处理更复杂的模式和抽象表示，但也增加了模型的深度和训练难度。\n",
    "\n",
    "6. **drop_rate** (0.1): Dropout比率，用于防止过拟合的正则化技术。在训练过程中随机\"丢弃\"10%的神经元，迫使网络学习更强健的特征。\n",
    "\n",
    "7. **qkv_bias** (False): 是否在查询(Q)、键(K)和值(V)的线性变换中包含偏置项。添加偏置可以增加模型的灵活性，但也增加了参数量。\n",
    "\n",
    "使用此配置，我们将实现一个简化的、用于演示或测试目的的 GPT架构（Dummy GPTModel），如图所示。这个示例模型展示了GPT的基本结构，包括词元嵌入、位置嵌入、Transformer块堆叠和输出层，但内部核心组件（如Transformer块）仅作为占位符。在后续小节中，我们将逐步实现这些核心组件。\n",
    "    \n",
    "<img src=\"./images_llm/fig4.3.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class DummyGPTModel(nn.Cell):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyGPTModel, self).__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "\n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.SequentialCell(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Dense(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], has_bias=False\n",
    "        )\n",
    "\n",
    "    def construct(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(ops.arange(seq_len))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Cell):\n",
    "    def __init__(self, cfg):\n",
    "        super(DummyTransformerBlock, self).__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def construct(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Cell):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super(DummyLayerNorm, self).__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def construct(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images_llm/fig4.4.svg\" width='600'>\n",
    "\n",
    "上图展示了GPT模型的基本数据流程。输入的词元ID首先被转换为嵌入向量，然后与位置嵌入相加以保留序列中的位置信息。经过Dropout处理后，数据流经多个Transformer块，每个块进一步处理和转换表示。最后，通过一个线性层将结果映射到词汇表大小的输出，生成每个可能词元的概率分布。\n",
    "\n",
    "在接下来的部分中，我们将逐步实现这个架构中的各个核心组件，最终组装成一个完整的、功能性的GPT模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6109 3626 6100  345]\n",
      " [6109 1110 6622  257]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from mindspore import Tensor\n",
    "# 指定本地词汇表文件所在的目录路径\n",
    "local_path = \"./gpt2-tokenizer\"\n",
    "\n",
    "# 从本地路径加载GPT - 2分词器\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(local_path)\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(Tensor(tokenizer.encode(txt1)))\n",
    "batch.append(Tensor(tokenizer.encode(txt2)))\n",
    "batch = ops.stack(batch, axis=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (2, 4, 50257)\n",
      "[[[ 3.4227250e-03  7.4077314e-03 -1.7565343e-03 ...  1.7987960e-04\n",
      "   -2.8054556e-04  6.2894425e-03]\n",
      "  [-1.6051477e-02 -6.9802068e-04  5.9053577e-03 ...  1.0963629e-02\n",
      "   -1.1396457e-02 -1.0390200e-02]\n",
      "  [-7.0738825e-03  7.5644581e-03 -9.0161264e-03 ... -8.5941190e-04\n",
      "   -2.0665708e-03 -4.1220831e-03]\n",
      "  [ 1.3805581e-03 -1.0841140e-02 -4.0423525e-03 ...  5.1172962e-04\n",
      "    2.9840767e-03 -2.8948567e-03]]\n",
      "\n",
      " [[ 3.4227250e-03  7.4077314e-03 -1.7565343e-03 ...  1.7987960e-04\n",
      "   -2.8054556e-04  6.2894425e-03]\n",
      "  [ 4.9724570e-04  2.1194271e-03  8.8037616e-03 ...  9.2625152e-05\n",
      "   -9.1164280e-03 -1.5153075e-02]\n",
      "  [-3.3303255e-03  4.1079315e-04 -4.1647851e-03 ...  3.4606953e-03\n",
      "    1.1678563e-03 -7.5262091e-03]\n",
      "  [-5.1538306e-03 -1.1113978e-02  1.1534748e-02 ...  5.0090058e-03\n",
      "    1.2650667e-02 -3.3001843e-04]]]\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 使用层归一化对激活值归一化\n",
    "\n",
    "深度神经网络在训练过程中经常面临梯度消失或爆炸等问题，这些问题会导致训练不稳定，收敛缓慢甚至失败。层归一化(Layer Normalization)是一种强大的技术，可以缓解这些问题，使深度网络训练更加稳定和高效。\n",
    "\n",
    "与批归一化(Batch Normalization)不同，层归一化在单个样本内部进行操作，而不是跨批次样本。这使得它特别适合处理序列数据和变长输入，因此在Transformer架构中被广泛采用。层归一化的核心思想是将每一层的激活值调整为均值为0、方差为1的标准分布，然后通过可学习的缩放和偏移参数进行调整，使网络能够学习最适合任务的数据分布。\n",
    "    \n",
    "\n",
    "<img src='./images_llm/fig4.5.svg' width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10539116 0.         0.         0.         0.4322223  0.        ]\n",
      " [0.2584788  0.         0.31767976 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "\n",
    "# create 2 training examples with 5 dimensions (features) each\n",
    "batch_example = ops.randn(2, 5) \n",
    "\n",
    "layer = nn.SequentialCell(nn.Dense(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " [[0.08960224]\n",
      " [0.09602642]]\n",
      "Variance:\n",
      " [[0.02495868]\n",
      " [0.01873421]]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.numpy as np\n",
    "\n",
    "mean = np.mean(out, axis=-1, keepdims=True)\n",
    "var = np.var(out, axis=-1, keepdims=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均值张量中，第一行表示第一个输入样本各输出的平均值，第二行则表示第二个输入样本各输出的平均值。在计算均值或方差等操作时，使用keepdim=True参数可以确保输出张量保持与输入张量相同的维度，以便于后续的广播操作。\n",
    "\n",
    "层归一化的计算过程可以简化为以下几个步骤：\n",
    "\n",
    "1. 计算每个样本特征的均值和方差\n",
    "2. 使用均值和方差对样本特征进行标准化(z-score归一化)\n",
    "3. 应用可学习的缩放和偏移参数，恢复表达能力\n",
    "\n",
    "下图展示了层归一化的数学计算过程，其中每一步都对应于上述的操作步骤：\n",
    "\n",
    "<img src=\"./images_llm/fig4.6.svg\" width='600'>\n",
    "\n",
    "对层输出进行归一化操作需要两个关键步骤：首先减去对应维度的平均值（中心化），然后除以方差的平方根（标准差）实现标准化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " [[ 0.09994053 -0.5671632  -0.5671632  -0.5671632   2.1687126  -0.5671632 ]\n",
      " [ 1.1868842  -0.70157325  1.6194091  -0.70157325 -0.70157325 -0.70157325]]\n",
      "Mean:\n",
      " [[3.973643e-08]\n",
      " [3.973643e-08]]\n",
      "Variance:\n",
      " [[0.99999994]\n",
      " [1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / np.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = np.mean(out_norm, axis=-1, keepdims=True)\n",
    "var = np.var(out_norm, axis=-1, keepdims=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "避免科学计数法显示非常小的数值，可以使用特定的打印选项，使结果更易于阅读："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " [[0.00000004]\n",
      " [0.00000004]]\n",
      "Variance:\n",
      " [[0.99999994]\n",
      " [1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.set_printoptions(suppress=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到目前为止，我们已经逐步实现了层归一化的核心计算过程。为了方便在后续模型中使用，我们将这个过程封装成一个完整的Mindspore模块。这样的封装不仅提高了代码的可读性和可重用性，也与深度学习框架的设计理念保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Cell):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = mindspore.Parameter(ops.ones(emb_dim, dtype=mindspore.float32))\n",
    "        self.shift = mindspore.Parameter(ops.zeros(emb_dim, dtype=mindspore.float32))\n",
    "\n",
    "    def construct(self, x):\n",
    "        mean = np.mean(x, axis=-1, keepdims=True)\n",
    "        var = np.var(x, axis=-1, keepdims=True)\n",
    "        norm_x = (x - mean) / np.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种特定的层规范化方法是针对输入张量$x$的最后一个维度进行操作的，该维度通常代表嵌入维度（emb_dim）。在规范化过程中，我们引入了一个极小常数 eps（即 epsilon），将其添加到方差中，以避免因方差为零而导致的除零错误，增强数值稳定性。\n",
    "\n",
    "层规范化还引入了两个可训练参数：比例参数（scale）和偏移参数（shift），它们的维度与输入张量的最后一个维度保持一致。这些参数允许模型学习每个特征的最佳缩放和偏移量，使得归一化后的表示能够保持足够的表达能力。在模型训练过程中，语言模型（LLM）会自动优化这些参数；如果调整它们能够提升模型在特定任务上的表现，模型就会学习到最适合当前数据的缩放和偏移方式。\n",
    "\n",
    "这种设计不仅增强了模型的灵活性，还使其能够更好地适应不同数据分布，从而提升整体性能。需要注意的是，层归一化在Transformer架构中起着至关重要的作用，它不仅稳定了训练过程，还加速了收敛速度。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " [[-0.00000007]\n",
      " [-0.00000001]]\n",
      "Variance:\n",
      " [[0.9999264]\n",
      " [0.9999944]]\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5) \n",
    "out_ln = ln(batch_example) \n",
    "mean = np.mean(out_ln, axis=-1, keepdims=True) \n",
    "var = np.var(out_ln, axis=-1, keepdims=True) \n",
    "print(\"Mean:\\n\", mean) \n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images_llm/fig4.7.svg\" width='600'>\n",
    "\n",
    "上图展示了层归一化在Transformer块中的应用位置。在现代GPT模型中，层归一化通常应用于多头注意力机制和前馈网络之前（称为Pre-LayerNorm），这种配置有助于稳定深度Transformer网络的训练。原始Transformer论文中采用的是Post-LayerNorm（在子模块后应用归一化），但研究表明Pre-LayerNorm对于训练非常深的模型更为有效，因此被广泛采用在现代架构中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 使用 GELU 激活实现前馈网络\n",
    "\n",
    "激活函数是神经网络中引入非线性的关键元素，使网络能够学习复杂的模式和关系。在Transformer架构中，前馈神经网络(Feed-Forward Network, FFN)层使用非线性激活函数处理经过注意力机制后的表示，进一步提取和转换特征。\n",
    "\n",
    "接下来，我们将着手实现一个前馈神经网络子模块，作为LLM中Transformer块的重要组成部分。我们首先实现GELU(Gaussian Error Linear Unit)激活函数，这是现代语言模型中最常用的激活函数之一。\n",
    "\n",
    "ReLU(Rectified Linear Unit)激活函数因其简洁性和有效性，在深度学习领域得到了广泛应用。然而，在大型语言模型中，除了传统的ReLU函数外，GELU和SwiGLU等更复杂的激活函数表现出了更优的性能。\n",
    "\n",
    "GELU和SwiGLU是更平滑的激活函数，GELU融合了高斯累积分布函数的特性，而SwiGLU则结合了S型（sigmoid）门控和线性单元。与简单的ReLU相比，这些更复杂的激活函数能够更好地捕捉数据的非线性特征，提供了在梯度传播和表达能力之间的平衡，因此在现代语言模型中被广泛采用。\n",
    "\n",
    "GELU激活函数的精确数学定义为 GELU(x)=xΦ(x)，其中 Φ(x) 是标准高斯分布的累积分布函数。为了计算效率，在实践中通常使用下面的近似公式，这也是GPT-2模型使用的版本：\n",
    "\n",
    "$$\n",
    "\\text{GELU}(x)  \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot (x + 0.044715 \\cdot x^3)\\right]\\right)\n",
    "$$\n",
    "\n",
    "这个近似公式通过曲线拟合得到，提供了计算上更为高效的实现方式，同时保持了GELU的关键特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(GELU, self).__init__()\n",
    "\n",
    "\n",
    "    def construct(self, x):\n",
    "        return 0.5 * x * (1 + np.tanh(\n",
    "            np.sqrt(Tensor(2.0 / np.pi)) * (x + 0.044715 * ops.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = ops.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU函数（位于右侧）是一个分段线性函数，当输入为正数时，它直接输出输入值；而当输入为负数时，则输出零。这种特性使得ReLU在负值区域的梯度为零，可能导致一些神经元\"死亡\"（永远不被激活）。\n",
    "\n",
    "相比之下，GELU函数（位于左侧）展现出一个平滑的非线性特性，形态上近似于ReLU，但对于负值输入也能提供非零的梯度（除了极少数点）。这种平滑特性使得GELU在不同输入范围内都能进行有效的梯度传播，减少了神经元死亡的问题，并对噪声输入具有更好的鲁棒性。\n",
    "\n",
    "GELU的这些特性使其在处理语言数据时表现出色，因为语言数据往往包含复杂的结构和关系，需要激活函数能够捕捉微妙的非线性模式。这也是为什么GELU成为现代语言模型中主流激活函数的原因之一。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Cell):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.SequentialCell(\n",
    "            nn.Dense(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Dense(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images_llm/fig4.8.svg\" width='400'>\n",
    "\n",
    "前馈网络是Transformer块中的关键组件，通常跟随在多头注意力机制之后。它由两个线性变换组成，中间使用非线性激活函数（在这里是GELU）连接。这种结构可以被解释为一个位置级的特征处理器，其工作流程如下：\n",
    "\n",
    "1. 第一个线性层将输入从嵌入维度扩展到更大的中间维度（通常是嵌入维度的4倍），增加了模型的表达能力\n",
    "2. GELU激活函数引入非线性，使网络能够学习复杂的模式\n",
    "3. 第二个线性层将维度压缩回原始的嵌入维度，整合处理后的特征\n",
    "\n",
    "这种设计允许模型在每个位置上独立地处理和转换特征，补充了多头注意力机制捕获的序列内依赖关系，从而增强了模型的整体表达能力。\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 768)\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = ops.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈模块可以增强模型从数据中学习和泛化的能力。它的特点是首先通过第一个线性层将嵌入维度扩展到更高维空间（通常是原始维度的4倍），这种扩展提供了更丰富的表达空间；随后应用非线性GELU激活，引入复杂的非线性变换；最后通过第二个线性变换将维度缩减回原始大小，整合和压缩学到的特征表示。\n",
    "\n",
    "简而言之，前馈网络可以看作是对每个位置表示的\"深度处理\"步骤，它补充了注意力机制的\"宽度处理\"（跨位置的信息交互），两者共同构成了Transformer块的强大学习能力。\n",
    "    \n",
    "<img src=\"./images_llm/fig4.9.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 添加快捷连接\n",
    "\n",
    "快捷连接（Skip Connection，也称为残差连接Residual Connection）是深度神经网络中的一项关键创新，最初由He等人在ResNet架构中提出。它解决了深度网络训练过程中的一个基本问题：随着网络深度增加，梯度在反向传播过程中趋于消失，导致浅层难以有效学习。\n",
    "\n",
    "这一问题在Transformer等深度架构中尤为严重，因为它们通常包含大量堆叠的层。快捷连接通过在网络中创建额外的路径，允许信息和梯度直接跳过一个或多个层，从而有效地缓解了梯度消失问题。\n",
    "\n",
    "如图所示，快捷连接的实现非常简单：将层的输入直接添加到该层的输出上。这种简单的加法操作创建了一条梯度流动的\"高速公路\"，使得深层网络更容易训练，同时保持了网络的表达能力。\n",
    "    \n",
    "<img src=\"./images_llm/fig4.10.svg\" width='600'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Cell):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.CellList([\n",
    "            nn.SequentialCell(nn.Dense(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.SequentialCell(nn.Dense(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.SequentialCell(nn.Dense(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.SequentialCell(nn.Dense(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.SequentialCell(nn.Dense(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def construct(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Compute the output of the current layer\n",
    "            layer_output = layer(x)\n",
    "            # Check if shortcut can be applied\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "\n",
    "    def forward(x):\n",
    "        # 前向传播\n",
    "        output = model(x)\n",
    "        target = Tensor([[0.]])\n",
    "\n",
    "        # 基于目标值和输出值计算损失\n",
    "        loss = nn.MSELoss()\n",
    "        loss = loss(output, target)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    # 反向传播\n",
    "    grad_fn = mindspore.value_and_grad(forward, None, weights=model.trainable_params())\n",
    "    loss, grads = grad_fn(x)\n",
    "    for param, grad in zip(model.trainable_params(), grads):\n",
    "        if 'weight' in param.name:\n",
    "            # 打印权重的平均绝对梯度\n",
    "            print(f\"{param.name} has gradient mean of {grad.abs().mean().asnumpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不添加快捷连接，即设置`use_shortcut=False`，我们会观察到深度网络中典型的梯度消失现象。随着层数的增加，浅层（如layers.0）的梯度值明显小于深层（如layers.4），这意味着较深层获得了更有效的更新，而浅层学习变得非常缓慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 5.775319368694909e-05\n",
      "layers.1.0.weight has gradient mean of 0.00013170905003789812\n",
      "layers.2.0.weight has gradient mean of 0.00036027480382472277\n",
      "layers.3.0.weight has gradient mean of 0.00284433807246387\n",
      "layers.4.0.weight has gradient mean of 0.07787459343671799\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = Tensor([[1., 0., -1.]])\n",
    "\n",
    "mindspore.set_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置`use_shortcut=True`激活快捷连接后，我们可以观察到显著的改进：浅层的梯度值得到了明显提升，与深层的差距大幅缩小。这表明快捷连接成功地允许梯度在网络中更加均匀地流动，使得网络的所有部分都能得到有效的学习。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.038417913019657135\n",
      "layers.1.0.weight has gradient mean of 0.03854157030582428\n",
      "layers.2.0.weight has gradient mean of 0.022813601419329643\n",
      "layers.3.0.weight has gradient mean of 0.03126213699579239\n",
      "layers.4.0.weight has gradient mean of 0.7397494912147522\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一层（`layers.4`）的梯度值仍然比其他层大，这是正常的，因为它直接连接到损失函数。然而，关键的改进是随着我们向第一层（`layers.0`）递进，梯度值保持在一个合理的范围内，没有急剧缩减至极小的值。这清晰地表明，快捷连接在缓解深度神经网络中的梯度消失问题方面发挥着至关重要的作用。\n",
    "\n",
    "在Transformer架构中，快捷连接被广泛应用于多头注意力机制和前馈网络模块。这些连接不仅使得非常深的模型（如拥有数十甚至上百层的大型语言模型）能够被有效训练，还为模型提供了学习身份映射的能力，使其能够在必要时\"跳过\"某些变换，增强了架构的灵活性和适应性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 在 transformer 模块中连接注意力层和线性层\n",
    "\n",
    "现在，我们已经实现了Transformer架构的核心组件：多头注意力机制、层归一化、前馈网络和快捷连接。接下来，我们将这些组件整合成完整的Transformer模块，这是GPT及其他现代语言模型的基本构建单元。\n",
    "\n",
    "Transformer模块是一个精心设计的组合，它有效地融合了信息处理的多个关键步骤：通过多头注意力机制捕获序列内的依赖关系，通过前馈网络处理每个位置的特征，并通过层归一化和快捷连接确保训练的稳定性和有效性。\n",
    "\n",
    "在拥有1.24亿参数的GPT-2架构中，这个模块被重复堆叠12次，每一层都在更抽象的层次上处理和转换序列表示。下图展示了Transformer模块的内部结构及其在GPT架构中的位置：\n",
    "    \n",
    "<img src=\"./images_llm/fig4.11.svg\" width='500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Cell):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Dense(d_in, d_out, has_bias=qkv_bias)\n",
    "        self.W_key = nn.Dense(d_in, d_out, has_bias=qkv_bias)\n",
    "        self.W_value = nn.Dense(d_in, d_out, has_bias=qkv_bias)\n",
    "        self.out_proj = nn.Dense(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 创建掩码矩阵\n",
    "        ones_matrix = ops.ones((context_length, context_length), mindspore.float32)\n",
    "        mask = ops.triu(ones_matrix, diagonal=1)\n",
    "        self.mask = mindspore.Parameter(mask, requires_grad=False)\n",
    "\n",
    "    def construct(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 重塑张量以分离头\n",
    "        keys = ops.reshape(keys, (b, num_tokens, self.num_heads, self.head_dim))\n",
    "        values = ops.reshape(values, (b, num_tokens, self.num_heads, self.head_dim))\n",
    "        queries = ops.reshape(queries, (b, num_tokens, self.num_heads, self.head_dim))\n",
    "\n",
    "        # 交换维度\n",
    "        keys = ops.transpose(keys, (0, 2, 1, 3))\n",
    "        queries = ops.transpose(queries, (0, 2, 1, 3))\n",
    "        values = ops.transpose(values, (0, 2, 1, 3))\n",
    "\n",
    "        # 计算注意力分数\n",
    "        keys_transposed = ops.transpose(keys, (0, 1, 3, 2))\n",
    "        attn_scores = ops.matmul(queries, keys_transposed)\n",
    "\n",
    "        # 准备掩码\n",
    "        mask_bool = self.mask[:num_tokens, :num_tokens].astype(mindspore.bool_)\n",
    "        neg_inf = mindspore.Tensor(-float('inf'), dtype=mindspore.float32)\n",
    "        attn_scores = ops.masked_fill(attn_scores, mask_bool, neg_inf)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        scale_factor = keys.shape[-1] ** 0.5\n",
    "        scaled_attn_scores = attn_scores / scale_factor\n",
    "        softmax = nn.Softmax(axis=-1)\n",
    "        attn_weights = softmax(scaled_attn_scores)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 计算上下文向量\n",
    "        context_vec = ops.matmul(attn_weights, values)\n",
    "        context_vec = ops.transpose(context_vec, (0, 2, 1, 3))\n",
    "        \n",
    "        # 合并头\n",
    "        context_vec = ops.reshape(context_vec, (b, num_tokens, self.d_out))\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from chapter03 import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Cell):\n",
    "    def __init__(self, cfg):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "\n",
    "    def construct(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码定义了一个 `TransformerBlock` 类，这是GPT模型的核心构建块。它采用了以下架构特性：\n",
    "\n",
    "1. **多头注意力机制(MultiHeadAttention)** ：使用多个注意力头并行处理输入，每个头关注序列中的不同模式。\n",
    "\n",
    "2. **前馈网络(FeedForward)** ：包含两个线性层和GELU激活函数，为每个位置独立地进行非线性特征变换。\n",
    "\n",
    "3. **层归一化(LayerNorm)** ：在每个子模块（注意力机制和前馈网络）之前应用，采用Pre-LayerNorm设计，与原始Transformer的Post-LayerNorm不同，这种设计在训练深层模型时表现更为稳定。\n",
    "\n",
    "4. **快捷连接(Shortcut Connection)** ：在每个子模块前后应用，允许梯度直接流过网络，缓解梯度消失问题，同时使模型能够保留输入信息。\n",
    "\n",
    "5. **Dropout**：应用于子模块的输出，随机\"丢弃\"一部分神经元，作为正则化技术防止过拟合。\n",
    "\n",
    "在前向传播过程中，输入数据首先通过层归一化，然后进入多头注意力机制。注意力机制的输出经过Dropout处理后，与原始输入相加（快捷连接）。然后，这个中间结果再次经过层归一化，进入前馈网络，其输出同样经过Dropout处理后与中间结果相加（又一个快捷连接）。这种精心设计的数据流使得深层Transformer网络能够有效学习和保持长距离依赖关系，同时保持训练的稳定性。\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用我们之前定义的 `GPT_CONFIG_124M` 字典实例化一个 `Transformer` 块并为其提供一些样本数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (2, 4, 768)\n",
      "Output shape: (2, 4, 768)\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "\n",
    "x = ops.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "可以看到，Transformer块保持了输入的形状（批次大小×序列长度×嵌入维度），但对内容进行了非线性变换。这种\"形状保持\"的特性使得Transformer块可以被方便地堆叠，构建任意深度的网络，而不需要额外的维度调整。在GPT模型中，多个相同的Transformer块依次处理数据，每一层都在前一层的基础上提取更高级别的特征和模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 GPT 模型\n",
    "\n",
    "在本章的开篇，我们对GPT架构进行了全面的概述，并引入了一个名为DummyGPTModel的示例实现。这个实现展示了GPT模型的整体结构，但其核心组件（如TransformerBlock和LayerNorm）仅作为没有实际功能的占位符。\n",
    "\n",
    "现在，我们已经实现了所有必要的组件，可以将它们组装成一个完整的、功能性的GPT模型。我们将使用之前编写的TransformerBlock和LayerNorm类，替代DummyGPTModel中的占位符，构建一个具有1.24亿参数的GPT-2模型。\n",
    "\n",
    "GPT模型的整体架构如下图所示，包括嵌入层、Transformer块堆叠和输出头。这种设计允许模型将输入文本转换为连续向量表示，通过多层Transformer处理捕获复杂的语言模式，最终生成每个可能词元的概率分布。\n",
    "\n",
    "<img src=\"./images_llm/fig4.12.svg\" width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现的GPTModel类具有以下关键组件：\n",
    "\n",
    "1. **词元嵌入(tok_emb)** ：将输入的词元ID转换为高维向量表示，捕获词元的语义特征。\n",
    "\n",
    "2. **位置嵌入(pos_emb)** ：为每个位置提供一个唯一的向量表示，使模型能够区分不同位置的相同词元。\n",
    "\n",
    "3. **嵌入Dropout(drop_emb)** ：应用于嵌入层的输出，作为正则化手段防止过拟合。\n",
    "\n",
    "4. **Transformer块堆叠(trf_blocks)** ：多个相同的Transformer块连续处理数据，每个块包含多头注意力机制和前馈网络。\n",
    "\n",
    "5. **最终层归一化(final_norm)** ：在输出头之前应用的归一化层，确保输出特征分布的稳定性。\n",
    "\n",
    "6. **输出头(out_head)** ：将最终的特征表示映射到词汇表大小的向量，每个元素表示对应词元的得分（logits）。\n",
    "\n",
    "在前向传播过程中，模型首先将输入ID转换为嵌入向量，并添加位置信息；然后经过一系列Transformer块的处理；最后通过线性层将结果映射到词汇表空间，得到每个可能词元的未归一化对数概率（logits）。这些logits可以直接用于训练（计算损失）或生成（通过采样下一个词）。\n",
    "\n",
    "这种端到端的架构设计使GPT模型能够理解并生成连贯、流畅的文本，捕捉语言中的长距离依赖关系和复杂语义模式，为各种自然语言处理任务提供强大的基础。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Cell):\n",
    "    def __init__(self, cfg):\n",
    "        super(GPTModel, self).__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(p=cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.SequentialCell(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Dense(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], has_bias=False\n",
    "        )\n",
    "\n",
    "    def construct(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_indices = Tensor(np.arange(seq_len), dtype=mindspore.int32)\n",
    "        pos_embeds = self.pos_emb(pos_indices)\n",
    "        pos_embeds = mindspore.ops.expand_dims(pos_embeds, 0)\n",
    "        pos_embeds = mindspore.ops.tile(pos_embeds, (batch_size, 1, 1))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将使用 `GPT_CONFIG_124M` 字典（作为cfg参数传入）来初始化一个具有1.24亿个参数的GPT模型，并将我们之前创建的批量文本输入提供给该模型进行处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " [[6109 3626 6100  345]\n",
      " [6109 1110 6622  257]]\n",
      "\n",
      "Output shape: (2, 4, 50257)\n",
      "[[[ 0.31178507 -0.5457619  -0.06643209 ...  0.28932837  0.5990323\n",
      "   -0.26291043]\n",
      "  [ 0.21411943 -0.48537663  0.1574881  ...  0.4713522   0.28291106\n",
      "   -0.22458228]\n",
      "  [ 0.03199475 -0.49083564  0.13941392 ...  0.4706279   0.30427632\n",
      "   -0.5071233 ]\n",
      "  [ 0.0748542  -0.47420925  0.11044836 ...  0.5467526   0.24440886\n",
      "   -0.5193511 ]]\n",
      "\n",
      " [[ 0.31178507 -0.5457619  -0.06643209 ...  0.28932837  0.5990323\n",
      "   -0.26291043]\n",
      "  [ 0.36740708 -0.54842234  0.10895526 ...  0.2772324   0.500062\n",
      "   -0.65989274]\n",
      "  [ 0.5164281  -0.8352239   0.29712898 ...  0.45427608  0.5243402\n",
      "   -0.78190607]\n",
      "  [ 0.5142338  -0.4501591   0.3219374  ...  0.5002439   0.27415723\n",
      "   -0.6930151 ]]]\n"
     ]
    }
   ],
   "source": [
    "mindspore.set_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过使用numel()方法（即\"number of elements\"的简称），我们可以统计出模型参数张量中的总参数个数。这个数字代表了模型的复杂度和表达能力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.trainable_params())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能会注意到一个有趣的差异：我们提到要初始化一个具有1.24亿个参数的GPT模型，但实际统计的参数数量却是1.63亿个。这种差异源于原始GPT-2架构中采用的一项称为权重绑定(Weight Tying)的技术。\n",
    "\n",
    "权重绑定是一种参数共享技术，它通过在模型的不同部分共享相同的权重矩阵来减少参数数量。具体来说，原始GPT-2模型在其输出层中重复使用了来自词元嵌入层的权重矩阵，即tok_emb和out_head使用相同的权重。这种设计基于一个观察：将词嵌入矩阵用作输出投影不仅可以减少参数数量，还能在语言模型训练中提高性能。\n",
    "\n",
    "这种权重共享不仅减少了参数数量，还提供了一种有用的归纳偏置，使模型在嵌入空间和输出空间中使用一致的表示。在完整的GPT-2实现中，这种权重绑定将我们观察到的1.63亿参数减少到了公开报告的1.24亿参数。\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: (50257, 768)\n",
      "Output layer shape: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.embedding_table.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果考虑权重绑定（即将tok_emb和out_head的权重共享），我们可以计算更准确的参数数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.get_parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个数字非常接近原始GPT-2论文中报告的1.24亿参数。一个有趣的观察是，即使拥有超过1亿的参数，模型大小也只有大约622MB。这突显了现代深度学习框架的效率，以及为什么更大规模的模型（如拥有1750亿参数的GPT-3）需要特殊的分布式训练技术和硬件资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# Convert to megabytes\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 生成文本 \n",
    "\n",
    "至此，我们已经构建了一个完整的GPT模型架构。现在，让我们探索如何使用这个模型生成新的文本。生成过程的核心是自回归生成：模型逐个预测序列中的下一个词元，然后将预测的词元添加到输入上下文中，形成一个迭代过程。\n",
    "\n",
    "在实际应用中，文本生成不仅涉及模型的前向传播，还包括从模型输出的概率分布中采样、处理特殊标记、实现停止条件等。下图展示了生成过程的基本流程：\n",
    "\n",
    "<img src=\"./images_llm/fig4.13.svg\" width='600'>\n",
    "\n",
    "GPT模型从输出张量到生成文本的过程涉及多个精细步骤，如图所示。首先，模型接收一个输入上下文（可能只是一个开始词或短语），然后计算词汇表中每个词元作为下一个词的概率。接下来，从这个概率分布中选择一个词元（可以是最高概率的词元，也可以使用温度采样等技术引入随机性）。选择的词元被添加到上下文中，然后重复这个过程，直到达到预定的长度或生成特殊的结束标记。\n",
    "\n",
    "<img src=\"./images_llm/fig4.14.svg\" width='600'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.context\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        # with mindspore.context.grad_off():\n",
    "        logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        softmax = ops.Softmax(axis=-1)\n",
    "        probas = softmax(logits)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = ops.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = ops.concat((idx, idx_next), axis=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们实现的`generate_text_simple`函数展示了这个过程的简化版本。该函数有以下关键步骤：\n",
    "\n",
    "1. **裁剪上下文**：如果当前上下文长度超过模型支持的最大上下文长度，只使用最后一部分作为有效上下文。\n",
    "\n",
    "2. **获取预测**：将当前上下文输入模型，获取所有可能的下一个词元的logits。\n",
    "\n",
    "3. **关注最后时间步**：只关注序列中最后一个位置的预测，因为我们只需要预测下一个词元。\n",
    "\n",
    "4. **转换为概率**：使用softmax函数将logits转换为概率分布。\n",
    "\n",
    "5. **选择下一个词元**：在这个简单实现中，直接选择概率最高的词元（贪婪解码）。在更复杂的实现中，可以使用温度采样、核采样等策略引入随机性和多样性。\n",
    "\n",
    "6. **更新上下文**：将选择的词元添加到当前上下文，为下一轮生成做准备。\n",
    "\n",
    "这个循环继续进行，直到生成指定数量的新词元。虽然这是一个简化的实现，但它捕捉了现代语言模型文本生成的核心流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"./images_llm/fig4.15.svg\" width='600'>\n",
    "\n",
    "图中展示了生成过程中的循环：模型接收上下文，预测下一个词元，将预测的词元添加到上下文中，然后重复这个过程。这种自回归生成方式使模型能够产生连贯的、上下文相关的文本输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们需要准备一个起始上下文作为生成的种子。这可以是一个短语、一个问题或者任何文本片段，用于引导模型开始生成过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: (1, 4)\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = Tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们使用实现的生成函数，从起始上下文开始生成新的文本。在生成过程中，我们关闭模型的Dropout（通过设置`model.set_train(False)`），因为Dropout是一种训练时的正则化技术，在推理阶段不需要应用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [[15496    11   314   716 10636 10636 48695 10636 48695 48695]]\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False) # 关闭 dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们将生成的整数ID序列解码回人类可读的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am deputy deputy vacations deputy vacations vacations\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0))\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，未经训练的GPT模型生成的文本是不连贯的，它重复生成\"deputy\"和\"vacations\"等词汇。这是因为模型的权重是随机初始化的，没有经过任何训练来学习语言的结构和规律。为了生成流畅、连贯和有意义的文本，模型需要在大规模文本语料库上进行预训练，学习语言的统计规律和模式。\n",
    "\n",
    "在实践中，生成过程可以通过多种技术进一步优化，如：\n",
    "\n",
    "- **温度采样(Temperature Sampling)** ：通过调整softmax函数的温度参数，控制生成文本的随机性和创造性\n",
    "- **Top-K采样**：只从概率最高的K个词元中采样，过滤掉低概率选项\n",
    "- **Top-p采样(Nucleus Sampling)** ：只从累积概率达到阈值p的最小词元集合中采样，平衡多样性和质量\n",
    "- **束搜索(Beam Search)** ：维护多个候选序列，最终选择整体概率最高的序列\n",
    "\n",
    "这些技术可以帮助模型生成更流畅、更多样、更有创意的文本，满足不同应用场景的需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 总结\n",
    "\n",
    "在本章中，我们从零开始实现了一个完整的GPT模型架构，并探索了其核心组件和工作原理。以下是本章的关键要点：\n",
    "\n",
    "- **层归一化(Layer Normalization)** 是深度神经网络训练的关键组件，它通过调整每一层的输出，使其均值和方差保持一致，从而稳定了训练过程、加速了收敛并提高了模型性能。在Transformer架构中，层归一化通常应用于子模块的输入（Pre-LayerNorm设计），这种配置比原始Transformer的Post-LayerNorm更适合训练非常深的网络。\n",
    "\n",
    "- **快捷连接(Skip Connections)** 是处理深度神经网络梯度消失问题的有效技术，通过直接将一层的输入添加到其输出，创建了梯度流动的\"高速公路\"。在GPT等模型中，这些连接对于训练数十甚至上百层的深度网络至关重要，确保了从输出到输入的有效梯度传播。\n",
    "\n",
    "- **GELU激活函数**是现代语言模型中的主流选择，它比ReLU提供更平滑的非线性特性，能够更好地处理语言数据中的复杂模式。在前馈网络中，GELU与线性层组合，提供了位置级别的特征处理，补充了注意力机制的序列级信息交互。\n",
    "\n",
    "- **Transformer块**是GPT模型的核心构建模块，它融合了多头因果注意力机制、前馈网络、层归一化和快捷连接。这个精心设计的模块能够有效地处理序列数据，捕捉长距离依赖关系，并且可以被堆叠到任意深度，构建强大的语言模型。\n",
    "\n",
    "- **GPT模型架构**由多层堆叠的Transformer块组成，从词元嵌入开始，通过位置编码添加序列信息，然后经过多层Transformer处理，最终通过线性层映射到词汇表空间。这种架构具有不同的规模变体，从1.24亿参数的小型模型到拥有数千亿参数的超大模型，都遵循相同的基本设计原则。\n",
    "\n",
    "- **文本生成**在GPT等自回归语言模型中是一个迭代过程，模型根据当前上下文预测下一个词元，然后将预测结果添加到上下文中，重复这个过程。通过不同的解码策略（如贪婪解码、温度采样、Top-K采样等），可以控制生成文本的多样性和创造性。\n",
    "\n",
    "- **模型训练**对于生成有意义的文本至关重要。未经训练的模型，无论架构多么精巧，都无法生成连贯的文本，因为它没有学习语言的结构和规律。只有通过在大规模语料库上的预训练，模型才能够捕捉语言的统计模式，生成流畅、有意义的内容。\n",
    "\n",
    "通过本章的实现，我们不仅掌握了GPT模型的技术细节和实现方法，还深入理解了现代大语言模型的基本工作原理。这些知识为我们理解、使用和改进现有的语言模型奠定了坚实的基础，同时也为我们探索更先进的模型架构和应用场景打开了大门。\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-1.15.0",
   "language": "python",
   "name": "tensorflow-1.15.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
